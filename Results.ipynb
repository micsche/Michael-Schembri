{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 No data Augmentation\n",
    "## No Data Augmentation – Errors 206/2226\n",
    "### All layers fixed - neural network\n",
    "\n",
    "Problem with OpenCV color transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/vgg16-no-data-aug.png\">\n",
    "<img src=\"files/files/vgg16-no-data-aug-loss.png\">\n",
    "<img src=\"files/files/vgg16-no-data-inference-example.png\">\n",
    "<img src=\"files/files/vgg16-no-data-inference-zoom.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16\n",
    "all_freezed_data_v2.4 \n",
    "\n",
    "Data Augmentation \n",
    "- rotation_range=40,        \n",
    "- shear_range=0.2,        \n",
    "- horizontal_flip=True,        \n",
    "- vertical_flip=True,        \n",
    "- fill_mode='nearest',        \n",
    "- zoom_range=0.2\n",
    "\n",
    "Dataset Shift on 9 locations\n",
    "\n",
    "OpenCV Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/vgg16-data-aug-train.png\">\n",
    "<img src=\"files/files/vgg16-data-aug-loss.png\">\n",
    "<img src=\"files/files/vgg16-data-aug-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset V3\n",
    "- UPAIR → each bounding box centered, cropped 224x224\n",
    "- 9 images created with  different centre positions within the crop\n",
    "- 70% / 30% training\n",
    "- Selection\n",
    "- TRASHNET → one image scaled to 224x224 - ALL\n",
    "\n",
    "### Background\n",
    "- 10 image sample from UPAIR per shot– vetted\n",
    "- Selection of Car/Stone\n",
    "\n",
    "V3 conv block -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset after DataGenerator\n",
    "\n",
    "# Data augmentation \n",
    "#### Train Set\n",
    "- rescale=1./255,\n",
    "- shear_range=0.2,\n",
    "- horizontal_flip=True,\n",
    "- vertical_flip=True,\n",
    "- fill_mode='nearest',\n",
    "- zoom_range=0.2)\n",
    "    \n",
    "#### validation set\n",
    "- rescale=1./255\n",
    "\n",
    "### Output\n",
    "- Found 7724 images belonging to 2 classes.\n",
    "- Found 3228 images belonging to 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Last Conv Block Training\n",
    "\n",
    "- VGG16 – InputLayer\t\t\t\tFalse\n",
    "- Conv2D Conv2D MaxPooling2D \t\tFalse\n",
    "- Conv2D Conv2D MaxPooling2D \t\tFalse\n",
    "- Conv2D Conv2D Conv2D MaxPooling2D False\n",
    "- Conv2D Conv2D Conv2D MaxPooling2D False\n",
    "- Conv2D Conv2D Conv2D MaxPooling2D True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>4096, activation=&#39;relu&#39;,name = &quot;fc1&quot;)</li>\n",
    "\t<li>Dropout(0.5)</li>\n",
    "\t<li>4096, activation=&#39;relu&#39;,name = &quot;fc2&quot;)</li>\n",
    "\t<li>Dropout(0.5)</li>\n",
    "\t<li>Dense(2, activation=&#39;softmax&#39;)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_batchsize = 100\n",
    "val_batchsize = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/vgg16-1conv-data-aug-loss.png\">\n",
    "<img src=\"files/files/vgg16-1conv-data-aug-train.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>VGG16 Last Conv Block Training</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Conv2D Conv2D MaxPooling2D False</li>\n",
    "\t<li>Conv2D Conv2D MaxPooling2D False</li>\n",
    "\t<li>Conv2D Conv2D Conv2D MaxPooling2D False</li>\n",
    "\t<li>Conv2D Conv2D Conv2D MaxPooling2D True</li>\n",
    "\t<li>Conv2D Conv2D Conv2D MaxPooling2D True</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/vgg16-fc1-fc2.4096-layer-8-train.png\">\n",
    "<img src=\"files/files/vgg16-fc1-fc2.4096-layer-8-loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>VGG16 Last Conv Block Training</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Conv2D Conv2D MaxPooling2D False</li>\n",
    "\t<li>Conv2D Conv2D MaxPooling2D False</li>\n",
    "\t<li>Conv2D Conv2D Conv2D MaxPooling2D True</li>\n",
    "\t<li>Conv2D Conv2D Conv2D MaxPooling2D True</li>\n",
    "\t<li>Conv2D Conv2D Conv2D MaxPooling2D True</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inceptionv3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Global Average Pooling\n",
    "- Fully Connected Layer 1024 - 1024\n",
    "- Categorical CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/inception-data-aug-gap10241024.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Global Average Pooling\n",
    "- Fully Connected Layer 2048 - 2048\n",
    "- Categorical CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/inception-data-aug-gap20482048.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAP \n",
    "- Fully Connected Layer 2048   \n",
    "- layers[172:] = layer.trainable = True\n",
    "- RMSprop(lr=1e-5)\n",
    "- Categorical CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/inception-data-aug-gap2048-layer172.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - GlobalAveragePooling2D()\n",
    "  - Dense(1024, activation='relu')\n",
    "  - layers[172:]:\n",
    "  - optimizer=SGD(lr=0.0001, momentum=0.9)\n",
    "  - loss='categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/inception-data-aug-gap1024-sgd-layer172.png\">\n",
    "<img src=\"files/files/inception-data-aug-gap1024-sgd-layer172-loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GlobalAveragePooling2D()(x)\n",
    "- Dense(1024, activation='relu')(x)\n",
    "- Dropout(0.5)(x)\n",
    "- Dense(1024, activation='relu')(x)\n",
    "- Dropout(0.5)(x)\n",
    "- layers[172:] layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/files/inception-data-aug-gap1024x2-layer172.png\">\n",
    "<img src=\"files/files/inception-data-aug-gap1024x2-layer172-loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
