@article{Chen2017,
abstract = {Existing object detection literature focuses on detecting a big object covering a large part of an image. The problem of detecting a small object covering a small part of an image is largely ignored. As a result, the state-of-the-art object detection algorithm renders unsatisfactory performance as applied to detect small objects in images. In this paper, we dedicate an effort to bridge the gap. We first compose a benchmark dataset tailored for the small object detection problem to better evaluate the small object detection performance. We then augment the state-of-the-art R-CNN algorithm with a context model and a small region proposal generator to improve the small object detection performance. We conduct extensive experimental validations for studying various design choices. Experiment results show that the augmented R-CNN algorithm improves the mean average precision by 29.8{\%} over the original R-CNN algorithm on detecting small objects.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Chen, Chenyi and Liu, Ming Yu and Tuzel, Oncel and Xiao, Jianxiong},
doi = {10.1007/978-3-319-54193-8_14},
eprint = {1311.2901},
file = {:home/mike/Downloads/TR2016-144.pdf:pdf},
isbn = {9783319541921},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {214--230},
pmid = {10463930},
title = {{R-CNN for small object detection}},
volume = {10115 LNCS},
year = {2017}
}
@article{Wang2010,
abstract = {For some applications, such as image recognition or compression, we cannot process the whole image directly for the reason that it is inefficient and unpractical. Therefore, several image segmentation algorithms were proposed to segment an im- age before recognition or compression. Image segmentation is to classify or cluster an image into several parts (regions) according to the feature of image, for example, the pixel value or the frequency response. Up to now, lots of image segmentation algo- rithms exist and be extensively applied in science and daily life. According to their segmentation method, we can approximately categorize them into region-based seg- mentation, data clustering, and edge-base segmentation. In this tutorial, we survey several popular image segmentation algorithms, discuss their specialties, and show their segmentation results. Moreover, some segmentation applications are described in the end.},
author = {Wang, Yh},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - 2010 - Tutorial Image Segmentation.pdf:pdf},
journal = {Tutorial, GraduateInstitute of {\ldots}},
pages = {1--36},
title = {{Tutorial Image Segmentation}},
year = {2010}
}
@article{Ando,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.07515v3},
author = {Ando, Shin and Huang, Chun Yuan},
eprint = {arXiv:1704.07515v3},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ando, Huang - Unknown - Deep Over-sampling Framework for Classifying Imbalanced Data.pdf:pdf},
keywords = {class imbalance,convolutional neural network,deep learn-,ing,representation learning,synthetic over-sampling},
pages = {1--16},
title = {{Deep Over-sampling Framework for Classifying Imbalanced Data}}
}
@article{Chawla2002,
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, N and Bowyer, K W and Hall, L O and Kegelmeyer, W P},
doi = {10.1613/jair.953},
eprint = {1106.1813},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chawla et al. - 2002 - {\{}SMOTE{\}} {\{}S{\}}ynthetic minority over-sampling technique.pdf:pdf},
isbn = {013805326X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {321--357},
pmid = {18190633},
title = {{{\{}SMOTE{\}}: {\{}S{\}}ynthetic minority over-sampling technique}},
volume = {16},
year = {2002}
}
@article{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank {\#}2 in object detection and {\#}3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-10578-9_23},
eprint = {1406.4729},
file = {:home/mike/Downloads/1406.4729.pdf:pdf},
isbn = {978-3-319-10577-2},
issn = {01628828},
pages = {1--14},
pmid = {26353135},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
url = {http://arxiv.org/abs/1406.4729{\%}0Ahttp://dx.doi.org/10.1007/978-3-319-10578-9{\_}23},
year = {2014}
}
@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
archivePrefix = {arXiv},
arxivId = {1606.05908},
author = {Doersch, Carl},
doi = {10.3389/fphys.2016.00108},
eprint = {1606.05908},
file = {:home/mike/Downloads/1606.05908.pdf:pdf},
isbn = {1532-4435},
issn = {1664042X},
keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
pages = {1--23},
pmid = {27148061},
title = {{Tutorial on Variational Autoencoders}},
url = {http://arxiv.org/abs/1606.05908},
year = {2016}
}
@article{DeVries2017,
abstract = {Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.},
archivePrefix = {arXiv},
arxivId = {1702.05538},
author = {DeVries, Terrance and Taylor, Graham W.},
eprint = {1702.05538},
file = {:home/mike/Downloads/1702.05538.pdf:pdf},
pages = {1--12},
title = {{Dataset Augmentation in Feature Space}},
url = {http://arxiv.org/abs/1702.05538},
year = {2017}
}
@article{Dodge2016,
abstract = {Image quality is an important practical challenge that is often overlooked in the design of machine vision systems. Commonly, machine vision systems are trained and tested on high quality image datasets, yet in practical applications the input images can not be assumed to be of high quality. Recently, deep neural networks have obtained state-of-the-art performance on many machine vision tasks. In this paper we provide an evaluation of 4 state-of-the-art deep neural network models for image classification under quality distortions. We consider five types of quality distortions: blur, noise, contrast, JPEG, and JPEG2000 compression. We show that the existing networks are susceptible to these quality distortions, particularly to blur and noise. These results enable future work in developing deep neural networks that are more invariant to quality distortions.},
archivePrefix = {arXiv},
arxivId = {1604.04004},
author = {Dodge, Samuel and Karam, Lina},
eprint = {1604.04004},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dodge, Karam - 2016 - Understanding How Image Quality Affects Deep Neural Networks.pdf:pdf},
isbn = {9781509003549},
title = {{Understanding How Image Quality Affects Deep Neural Networks}},
url = {http://arxiv.org/abs/1604.04004},
year = {2016}
}
@article{VanNoord2017,
abstract = {Convolutional Neural Networks (CNNs) require large image corpora to be trained on classification tasks. The variation in image resolutions, sizes of objects and patterns depicted, and image scales, hampers CNN training and performance, because the task-relevant information varies over spatial scales. Previous work attempting to deal with such scale variations focused on encouraging scale-invariant CNN representations. However, scale-invariant representations are incomplete representations of images, because images contain scale-variant information as well. This paper addresses the combined development of scale-invariant and scale-variant representations. We propose a multi-scale CNN method to encourage the recognition of both types of features and evaluate it on a challenging image classification task involving task-relevant characteristics at multiple scales. The results show that our multi-scale CNN outperforms single-scale CNN. This leads to the conclusion that encouraging the combined development of a scale-invariant and scale-variant representation in CNNs is beneficial to image recognition performance.},
archivePrefix = {arXiv},
arxivId = {1602.01255},
author = {van Noord, Nanne and Postma, Eric},
doi = {10.1016/j.patcog.2016.06.005},
eprint = {1602.01255},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van Noord, Postma - 2017 - Learning scale-variant and scale-invariant features for deep image classification.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Artist Attribution,Convolutional Neural Networks,Multi-scale,Scale-variant Features},
pages = {583--592},
title = {{Learning scale-variant and scale-invariant features for deep image classification}},
volume = {61},
year = {2017}
}
@article{Kauderer-Abrams2017,
abstract = {A fundamental problem in object recognition is the development of image representations that are invariant to common transformations such as translation, rotation, and small deformations. There are multiple hypotheses regarding the source of translation invariance in CNNs. One idea is that translation invariance is due to the increasing receptive field size of neurons in successive convolution layers. Another possibility is that invariance is due to the pooling operation. We develop a simple a tool, the translation-sensitivity map, which we use to visualize and quantify the translation-invariance of various architectures. We obtain the surprising result that architectural choices such as the number of pooling layers and the convolution filter size have only a secondary effect on the translation-invariance of a network. Our analysis identifies training data augmentation as the most important factor in obtaining translation-invariant representations of images using convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1801.01450},
author = {Kauderer-Abrams, Eric},
eprint = {1801.01450},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kauderer-Abrams - 2017 - Quantifying Translation-Invariance in Convolutional Neural Networks.pdf:pdf},
title = {{Quantifying Translation-Invariance in Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1801.01450},
year = {2017}
}
@article{Gurumurthy2017,
abstract = {A class of recent approaches for generating images, called Generative Adversarial Networks (GAN), have been used to generate impressively realistic images of objects, bedrooms, handwritten digits and a variety of other image modalities. However, typical GAN-based approaches require large amounts of training data to capture the diversity across the image modality. In this paper, we propose DeLiGAN -- a novel GAN-based architecture for diverse and limited training data scenarios. In our approach, we reparameterize the latent generative space as a mixture model and learn the mixture model's parameters along with those of GAN. This seemingly simple modification to the GAN framework is surprisingly effective and results in models which enable diversity in generated samples although trained with limited data. In our work, we show that DeLiGAN can generate images of handwritten digits, objects and hand-drawn sketches, all using limited amounts of data. To quantitatively characterize intra-class diversity of generated samples, we also introduce a modified version of "inception-score", a measure which has been found to correlate well with human assessment of generated samples.},
archivePrefix = {arXiv},
arxivId = {1706.02071},
author = {Gurumurthy, Swaminathan and Sarvadevabhatla, Ravi Kiran and Babu, R. Venkatesh},
doi = {10.1109/CVPR.2017.525},
eprint = {1706.02071},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gurumurthy, Sarvadevabhatla, Babu - 2017 - DeLiGAN Generative adversarial networks for diverse and limited data.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {4941--4949},
title = {{DeLiGAN : Generative adversarial networks for diverse and limited data}},
volume = {2017-Janua},
year = {2017}
}
@misc{Li2015,
address = {Stanford},
author = {Li, Fei-Fei and Karpathy, Andrej},
publisher = {Stanford University},
title = {{Data Augmentation - Stanford Vision Lab}},
url = {https://www.google.com/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=2{\&}ved=2ahUKEwjb3aLSoYHeAhWuwosKHf{\_}3BdYQFjABegQICBAC{\&}url=http{\%}3A{\%}2F{\%}2Fvision.stanford.edu{\%}2Fteaching{\%}2Fcs231n{\%}2Fslides{\%}2F2015{\%}2Flecture10.pdf{\&}usg=AOvVaw0artcWpkZKtb3G5E1iwvys},
year = {2015}
}
@article{Zhong2017,
abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
archivePrefix = {arXiv},
arxivId = {1708.04896},
author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
eprint = {1708.04896},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhong et al. - 2017 - Random Erasing Data Augmentation.pdf:pdf},
title = {{Random Erasing Data Augmentation}},
url = {http://arxiv.org/abs/1708.04896},
year = {2017}
}
@article{Wong2016,
abstract = {In this paper we investigate the benefit of augmenting data with synthetically created samples when training a machine learning classifier. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the benefits of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classifier, using the standard MNIST handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater benefit for improving performance and reducing overfitting.},
archivePrefix = {arXiv},
arxivId = {1609.08764},
author = {Wong, Sebastien C. and Gatt, Adam and Stamatescu, Victor and McDonnell, Mark D.},
doi = {10.1109/DICTA.2016.7797091},
eprint = {1609.08764},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong et al. - 2016 - Understanding Data Augmentation for Classification When to Warp.pdf:pdf},
isbn = {9781509028962},
journal = {2016 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2016},
title = {{Understanding Data Augmentation for Classification: When to Warp?}},
year = {2016}
}
@article{Pan2010,
abstract = {A survey on transfer learning. IEEE Trans. Knowl. Data Eng},
archivePrefix = {arXiv},
arxivId = {PAI},
author = {Pan, Sinno Jialin and Yang, Qiang},
doi = {10.1109/TKDE.2009.191},
eprint = {PAI},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Yang - 2010 - A survey on transfer learning.pdf:pdf},
isbn = {1041-4347 VO - 22},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Transfer learning,data mining.,machine learning,survey},
number = {10},
pages = {1345--1359},
title = {{A survey on transfer learning}},
volume = {22},
year = {2010}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
doi = {10.1109/IJCNN.2016.7727519},
eprint = {1411.1792},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - How transferable are features in deep neural networks.pdf:pdf},
isbn = {978-1-5090-0620-5},
issn = {10495258},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
volume = {27},
year = {2014}
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1212.5701},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:pdf},
isbn = {1212.5701},
issn = {09252312},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
doi = {10.1111/j.0006-341X.1999.00591.x},
eprint = {1609.04747},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruder - 2016 - An overview of gradient descent optimization algorithms.pdf:pdf},
isbn = {1541-0420},
issn = {0006341X},
pages = {1--14},
pmid = {11318219},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@article{Krizhevsky2010,
abstract = {We describe how to train a two-layer convolutional Deep Belief Network (DBN) on the 1.6 million tiny images dataset. When training a convolutional DBN, one must decide what to do with the edge pixels of teh images. As the pixels near the edge of an image contribute to the fewest convolutional filter outputs, the model may see it fit to tailor its few convolutional filters to better model the edge pixels. This is undesirable becaue it usually comes at the expense of a good model for the interior parts of the image. We investigate several ways of dealing with the edge pixels when training a convolutional DBN. Using a combination of locally-connected convolutional units and globally-connected units, as well as a few tricks to reduce the effects of overfitting, we achieve state-of-the-art performance in the classification task of the CIFAR-10 subset of the tiny images dataset.},
author = {Krizhevsky, Alex and Hinton, G},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Hinton - 2010 - Convolutional deep belief networks on cifar-10.pdf:pdf},
journal = {Unpublished manuscript},
pages = {1--9},
title = {{Convolutional deep belief networks on cifar-10}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Convolutional+Deep+Belief+Networks+on+CIFAR-10{\#}0},
year = {2010}
}
@article{Xie2017,
abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
archivePrefix = {arXiv},
arxivId = {1611.05431},
author = {Xie, Saining and Girshick, Ross and Doll{\'{a}}r, Piotr and Tu, Zhuowen and He, Kaiming},
doi = {10.1109/CVPR.2017.634},
eprint = {1611.05431},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie et al. - 2017 - Aggregated residual transformations for deep neural networks.pdf:pdf},
isbn = {9781538604571},
issn = {1063-6919},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5987--5995},
title = {{Aggregated residual transformations for deep neural networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Sandler,
archivePrefix = {arXiv},
arxivId = {arXiv:1801.04381v3},
author = {Sandler, Mark and Zhu, Menglong and Zhmoginov, Andrey and Apr, C V},
eprint = {arXiv:1801.04381v3},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sandler et al. - Unknown - MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf:pdf},
title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}}
}
@article{Howard2017,
abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
archivePrefix = {arXiv},
arxivId = {1704.04861},
author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
doi = {arXiv:1704.04861},
eprint = {1704.04861},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pmid = {23459267},
title = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}},
url = {http://arxiv.org/abs/1704.04861},
year = {2017}
}
@misc{Salton1983,
abstract = {The text begins with an introduction and a description of the main retrieval processes incorporated into existing, operational systems based on keyword indexing and Boolean query formulations (Chapters 1 and 2). Chapter 3 contains a detailed explanation of modern automatic indexing techniques with evaluation results and assessments of the importance and practical usefulness of the techniques. Experimental retrieval systems, based in part on fully automatic analysis, search, and retrieval methods, are covered in Chapter 4 with emphasis on the design of the SMART and SIRE systems developed by the authors. The main evaluation techniques used to assess the effectiveness and efficiency of information retrieval systems are covered in Chapter 5 with emphasis on the use of the well-known recall and precision measures. Chapter 6 deals with important techniques usable in the design of future systems, such as automatic classification methods, query negotiation, and reformulation processes used i},
author = {Salton, Gerard and McGill, Michael J.},
booktitle = {Introduction to modern information retrieval},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, McGill - 1983 - Introduction to modern information retrieval.pdf:pdf},
isbn = {0070544840},
issn = {0070544840},
keywords = {Automation,Boolean functions,INDEXING,Information retrieval systems},
pages = {400},
title = {{Introduction to modern information retrieval.}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=lxh{\&}AN=ISTA2001897{\&}site=ehost-live},
year = {1983}
}
@article{Antonets,
author = {Antonets, Mikhail A and Kogan, Grigoriy P},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Antonets, Kogan - Unknown - The variational principle for weights characterizing the relevance.pdf:pdf},
pages = {1--5},
title = {{The variational principle for weights characterizing the relevance}}
}
@article{Everingham2010,
abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
author = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Everingham et al. - 2010 - The pascal visual object classes (VOC) challenge.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition},
number = {2},
pages = {303--338},
pmid = {20713396},
title = {{The pascal visual object classes (VOC) challenge}},
volume = {88},
year = {2010}
}
@article{Kornblith,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.08974v1},
author = {Kornblith, Simon},
eprint = {arXiv:1805.08974v1},
file = {:home/mike/Downloads/1805.08974.pdf:pdf},
pages = {1--19},
title = {{Do Better ImageNet Models Transfer Better ?}}
}
@article{Uijlings2012,
abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce Selective Search which combines the strength of both an exhaustive search and seg-mentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our Selective Search results in a small set of data-driven, class-independent, high quality locations, yielding 99{\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The Selective Search software is made publicly available 1 .},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Uijlings, J R R and {Van De Sande}, K E A and Gevers, T and Smeulders, A W M},
doi = {10.1109/ICCV.2011.6126456},
eprint = {1409.4842},
isbn = {9781457711015},
issn = {1573-1405},
keywords = {()},
pmid = {24920543},
title = {{Selective Search for Object Recognition}},
url = {http://disi.unitn.it/},
year = {2012}
}
@misc{DeidunAlan2018Oblm,
author = {Deidun, Alan and Gauci, Adam and Lagorio, S and Galgani, Francois G},
keywords = {Marine debris -- Malta; Marine debris -- Malta --},
publisher = {Elsevier Ltd.},
title = {{Optimising beached litter monitoring protocols through aerial imagery}},
year = {2018}
}
@article{Wan2013,
abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.},
archivePrefix = {arXiv},
arxivId = {1509.08985},
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and LeCun, Yann and Fergus, Rob},
doi = {10.1109/TPAMI.2017.2703082},
eprint = {1509.08985},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wan et al. - 2013 - Regularization of neural networks using dropconnect.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {0162-8828},
journal = {Icml},
number = {1},
pages = {109--111},
pmid = {797520},
title = {{Regularization of neural networks using dropconnect}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013{\_}wan13},
year = {2013}
}
@misc{tensorflow2015-whitepaper,
annote = {Software available from tensorflow.org},
author = {{Martin Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and Craig{\~{}}Citro and {Greg{\~{}}S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and Rafal{\~{}}Jozefowicz and Lukasz{\~{}}Kaiser and Manjunath{\~{}}Kudlur and {Josh Levenberg} and {Dandelion Man{\'{e}}} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Vi{\'{e}}gas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
title = {{{\{}TensorFlow{\}}: Large-Scale Machine Learning on Heterogeneous Systems}},
url = {https://www.tensorflow.org/},
year = {2015}
}
@misc{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois and Others},
howpublished = {$\backslash$url{\{}https://keras.io{\}}},
title = {{Keras}},
year = {2015}
}
@misc{Thung2017,
author = {Thung, Gary},
title = {{Dataset of images of trash}},
url = {https://github.com/garythung/trashnet},
year = {2017}
}
@article{Tieleman2012,
abstract = {Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012},
author = {Tieleman, Tijmen and Hinton, Geoffrey E. and Srivastava, Nitish and Swersky, Kevin},
doi = {https://www.coursera.org/learn/neural-networks/lecture/YQHki/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tieleman et al. - 2012 - Lecture 6.5-rmsprop Divide the gradient by a running average of its recent magnitude.pdf:pdf},
journal = {COURSERA: Neural Networks for Machine Learning},
pages = {26----31},
title = {{Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude}},
url = {http://www.cs.toronto.edu/{~}tijmen/csc321/slides/lecture{\_}slides{\_}lec6.pdf},
volume = {4},
year = {2012}
}
@article{LeCun2006,
abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods. Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all possible variable configurations. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in the design of architectures and training criteria than probabilistic approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu Jie},
doi = {10.1198/tech.2008.s913},
eprint = {arXiv:1011.1669v3},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 2006 - A Tutorial on Energy-Based Learning.pdf:pdf},
isbn = {9780262026178},
issn = {{\textless}null{\textgreater}},
journal = {Predicting Structured Data},
pages = {191--246},
pmid = {25246403},
title = {{A Tutorial on Energy-Based Learning}},
year = {2006}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@article{Selvaraju2017,
abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.},
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
doi = {10.1109/ICCV.2017.74},
eprint = {1610.02391},
file = {:opt/pdf/1610.02391.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {618--626},
title = {{Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}},
volume = {2017-Octob},
year = {2017}
}
@article{Krishna,
abstract = {While the problem of detecting generic objects in natural scene images has been the subject of research for a long time, the problem of detection of small objects has been largely ignored. While generic object detectors perform well on medium and large sized objects, they perform poorly for the overall task of recognition of small objects. This is because of the low resolution and simple shape of most small objects. In this work, we suggest a simple yet effective upsampling-based technique that performs better than the current state-of-the-art for end-to-end small object detection. Like most recent methods, we generate proposals and then classify them. We suggest improvements to both these steps for the case of small objects.},
author = {Krishna, Harish and Jawahar, C V},
file = {:opt/pdf/Improving-SmallObject-Detection.pdf:pdf},
keywords = {object detection,super-resolution},
title = {{Improving Small Object Detection}},
url = {http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2017/Improving-SmallObject-Detection.pdf}
}
@article{Zhou2014,
abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
archivePrefix = {arXiv},
arxivId = {1412.6856},
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
doi = {10.1111/j.1651-2227.1979.tb18450.x},
eprint = {1412.6856},
file = {:opt/pdf/1412.6856.pdf:pdf},
isbn = {0001-656X (Print)$\backslash$r0001-656X (Linking)},
issn = {16512227},
pmid = {525342},
title = {{Object Detectors Emerge in Deep Scene CNNs}},
url = {http://arxiv.org/abs/1412.6856},
year = {2014}
}
@article{Kong2016,
abstract = {Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing.},
archivePrefix = {arXiv},
arxivId = {1604.00600},
author = {Kong, Tao and Yao, Anbang and Chen, Yurong and Sun, Fuchun},
doi = {10.1109/CVPR.2016.98},
eprint = {1604.00600},
file = {:opt/pdf/07780467.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
pmid = {4520227},
title = {{HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection}},
url = {http://arxiv.org/abs/1604.00600},
year = {2016}
}
@article{Rottensteiner,
author = {Rottensteiner, F and Sohn, G and {\ldots}, J Jung - {\ldots} . Remote Sens. Spat and undefined 2012},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rottensteiner et al. - Unknown - The ISPRS benchmark on urban object classification and 3D building reconstruction.pdf:pdf},
journal = {ipi.uni-hannover.de},
title = {{The ISPRS benchmark on urban object classification and 3D building reconstruction}},
url = {https://www.ipi.uni-hannover.de/uploads/tx{\_}tkpublikationen/isprsannals-I-3-293-2012.pdf}
}
@article{Tanner,
author = {Tanner, F and Colder, B and Pullen, C and {\ldots}, D Heagy - 2009 IEEE Applied and undefined 2009},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tanner et al. - Unknown - Overhead imagery research data set—An annotated data library {\&}amp tools to aid in the development of compute.pdf:pdf},
journal = {computer.org},
title = {{Overhead imagery research data set—An annotated data library {\&} tools to aid in the development of computer vision algorithms}},
url = {https://www.computer.org/csdl/proceedings/aipr/2009/5146/00/05466304-abs.html}
}
@article{Razakarivony2015,
abstract = {This paper introduces VEDAI: Vehicle Detection in Aerial Imagery a new database of aerial images provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit different variabilities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by different people can be properly reproduce and compared. Finally, the paper also gives the performance of baseline algorithms on this dataset, for different settings of these algorithms, to illustrate the difficulties of the task and provide baseline comparisons.},
author = {Razakarivony, S{\'{e}}bastien and Jurie, Frederic},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Aerial imagery,Computer vision,Database,Detection,Infrared imagery,Low resolution images,Target detection,Vehicles},
pages = {187--203},
title = {{Vehicle Detection in Aerial Imagery : A Small Target Detection Benchmark}},
volume = {34},
year = {2015}
}
@article{Everingham10,
abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
author = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition},
month = {jun},
number = {2},
pages = {303--338},
pmid = {20713396},
title = {{The pascal visual object classes (VOC) challenge}},
volume = {88},
year = {2010}
}
@book{Salton:1986:IMI:576628,
address = {New York, NY, USA},
author = {Salton, Gerard and McGill, Michael J},
isbn = {0070544840},
publisher = {McGraw-Hill, Inc.},
title = {{Introduction to Modern Information Retrieval}},
year = {1986}
}
@techreport{Downey2017,
author = {Downey, Rosemarie (Euromonitor)},
institution = {Euromonitor},
title = {{Global Packaging Market in 2017: Emerging Markets and Pack Size Variation}},
url = {https://blog.euromonitor.com/2017/07/packaging-market-2017.html},
year = {2017}
}
@article{Lecun2009,
author = {Lecun, Y},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun - 2009 - A Theoretical Analysis of Feature Pooling in Visual Recognition.pdf:pdf},
journal = {Citeseer},
number = {November},
title = {{A Theoretical Analysis of Feature Pooling in Visual Recognition}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.170.864},
year = {2009}
}
@techreport{Corbin,
author = {Corbin, Tony},
institution = {Packaging News},
title = {{Euromonitor data shows PET packaging growth}},
url = {https://www.packagingnews.co.uk/news/materials/rigid-plastics/euromonitor-data-shows-pet-packaging-growth-29-06-2017}
}
@inproceedings{lin2014microsoft,
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C Lawrence},
booktitle = {European conference on computer vision},
organization = {Springer},
pages = {740--755},
title = {{Microsoft coco: Common objects in context}},
year = {2014}
}
@article{openimages,
author = {Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Ferrari, Vittorio and Abu-El-Haija, Sami and Kuznetsova, Alina and Rom, Hassan and Uijlings, Jasper and Popov, Stefan and Kamali, Shahab and Malloci, Matteo and Pont-Tuset, Jordi and Veit, Andreas and Belongie, Serge and Gomes, Victor and Gupta, Abhinav and Sun, Chen and Chechik, Gal and Cai, David and Feng, Zheyun and Narayanan, Dhyanesh and Murphy, Kevin},
journal = {Dataset available from https://storage.googleapis.com/openimages/web/index.html},
title = {{OpenImages: A public dataset for large-scale multi-label and multi-class image classification.}},
year = {2017}
}
@article{Krizhevsky,
abstract = {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
author = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
keywords = {Dataset},
title = {{CIFAR-10 (Canadian Institute for Advanced Research)}},
url = {http://www.cs.toronto.edu/{~}kriz/cifar.html}
}
@inproceedings{imagenet_cvpr09,
author = {Deng, J and Dong, W and Socher, R and Li, L.-J. and Li, K and Fei-Fei, L},
booktitle = {CVPR09},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
year = {2009}
}
@article{Jarrett2009,
abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63{\%} recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6{\%}) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ({\&}gt; 65{\%}), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53{\%}).},
archivePrefix = {arXiv},
arxivId = {1602.05473},
author = {Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
doi = {10.1109/ICCV.2009.5459469},
eprint = {1602.05473},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jarrett et al. - 2009 - What is the best multi-stage architecture for object recognition.pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2146--2153},
pmid = {25719670},
title = {{What is the best multi-stage architecture for object recognition?}},
year = {2009}
}
@article{Teh,
author = {Teh, Yee Whye and Hinton, Geoffrey E},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teh, Hinton - Unknown - Rate-coded Restricted Boltzmann Machines for Face Recognition.pdf:pdf},
title = {{Rate-coded Restricted Boltzmann Machines for Face Recognition}}
}
@article{Pedamonti2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.02763v1},
author = {Pedamonti, Dabal},
eprint = {arXiv:1804.02763v1},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedamonti - 2013 - Comparison of non-linear activation functions for deep neural networks on.pdf:pdf},
number = {3},
title = {{Comparison of non-linear activation functions for deep neural networks on}},
year = {2013}
}
@article{Klambauer2017,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
doi = {1706.02515},
eprint = {1706.02515},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf:pdf},
issn = {10495258},
title = {{Self-Normalizing Neural Networks}},
url = {http://arxiv.org/abs/1706.02515},
year = {2017}
}
@article{Xu2015,
abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68$\backslash${\%} accuracy on CIFAR-100 test set without multiple test or ensemble.},
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in Convolutional Network.pdf:pdf},
title = {{Empirical Evaluation of Rectified Activations in Convolutional Network}},
url = {http://arxiv.org/abs/1505.00853},
year = {2015}
}
@article{kingmaAdam,
abstract = {Login systems in smart devices demand multi-factor authentication for high security and at the same time, it requires simple user experience. We propose a novel application of lip-reading satisfying these requirements. We present the adequacy of lip-reading as a biometric factor by experiment. In addition, automatic lip-reader can be implemented by LSTM (Long Short Term Memory) neural network architecture with good accuracy that can translate visual utterance to password as a knowledge factor. Furthermore, our proposed method, iterative method, can improve accuracy as much as login system required. Our work achieved 93.8{\%} by single iteration from the first result (69.1{\%}).},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Lee, Daehyun and Myung, Kyungsik},
doi = {10.1109/ICCE.2017.7889386},
eprint = {1412.6980},
isbn = {9781509055449},
issn = {09252312},
journal = {2017 IEEE International Conference on Consumer Electronics, ICCE 2017},
pages = {434--435},
pmid = {172668},
title = {{Read my lips, login to the virtual world}},
url = {http://arxiv.org/abs/1412.6980},
volume = {abs/1412.6},
year = {2017}
}
@article{Lee2017,
abstract = {Login systems in smart devices demand multi-factor authentication for high security and at the same time, it requires simple user experience. We propose a novel application of lip-reading satisfying these requirements. We present the adequacy of lip-reading as a biometric factor by experiment. In addition, automatic lip-reader can be implemented by LSTM (Long Short Term Memory) neural network architecture with good accuracy that can translate visual utterance to password as a knowledge factor. Furthermore, our proposed method, iterative method, can improve accuracy as much as login system required. Our work achieved 93.8{\%} by single iteration from the first result (69.1{\%}).},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Lee, Daehyun and Myung, Kyungsik},
doi = {10.1109/ICCE.2017.7889386},
eprint = {1412.6980},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Myung - 2017 - Read my lips, login to the virtual world.pdf:pdf},
isbn = {9781509055449},
issn = {09252312},
journal = {2017 IEEE International Conference on Consumer Electronics, ICCE 2017},
pages = {434--435},
pmid = {172668},
title = {{Read my lips, login to the virtual world}},
url = {http://arxiv.org/abs/1412.6980},
year = {2017}
}
@article{polyak,
abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, ..., xn, ..., which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ≤ t ≤ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1we use k previous iterations xn, ..., xn-k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0. {\textcopyright} 1964.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.4373v1},
author = {Polyak, B. T.},
doi = {10.1016/0041-5553(64)90137-5},
eprint = {arXiv:1305.4373v1},
issn = {00415553},
journal = {USSR Computational Mathematics and Mathematical Physics},
number = {5},
pages = {1--17},
title = {{Some methods of speeding up the convergence of iteration methods}},
volume = {4},
year = {1964}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
eprint = {arXiv:1011.1669v3},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Pmlr},
pages = {249--256},
pmid = {25246403},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Cybenko1993,
abstract = {Every function of bounded variation can be approximated by a linear combination of a fixed sigmoidal function of n terms to an error 0(1/n) in the uniform norm. {\textcopyright} 1993 Academic Press, Inc.},
author = {Cybenko, G},
doi = {10.1007/BF02836480},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cybenko - 1993 - Degree of approximation by superpositions of a sigmoidal function.pdf:pdf},
isbn = {0780300564},
issn = {10009221},
journal = {Approximation Theory and its Applications},
keywords = {approximation,completeness,neural networks},
number = {3},
pages = {17--28},
title = {{Degree of approximation by superpositions of a sigmoidal function}},
volume = {9},
year = {1993}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@misc{Karpathy2018,
author = {Karpathy, A},
title = {{CS231 Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/},
urldate = {2018-07-31},
year = {2018}
}
@book{Armington2011,
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Armington, John and Ho, Purdy and Koznek, Paul and Hewlett, Richard Martinez},
doi = {10.1007/978-3-642-23196-4},
eprint = {9780201398298},
isbn = {9783642231964},
issn = {03029743},
number = {December 2016},
pmid = {4520227},
title = {{Lecture Notes in Computer Science}},
year = {2011}
}
@article{Krogh1992,
abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk. 1 INTRODUCTION Many recent studies have shown that the generalization ability of a neural network (or any other `learning machine') depends on a balance between the information in the training examples and the complexity of the network, see for instance [1,2,3]. Bad generalization oc...},
author = {Krogh, A. and Hertz, J. A.},
doi = {https://dl.acm.org/citation.cfm?id=2987033},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krogh, Hertz - 1992 - A Simple Weight Decay Can Improve Generalization.pdf:pdf},
isbn = {1558602224},
journal = {Advances in Neural Information Processing Systems},
pages = {950--957},
title = {{A Simple Weight Decay Can Improve Generalization}},
url = {http://0-citeseerx.ist.psu.edu.innopac.up.ac.za/viewdoc/summary?doi=10.1.1.41.2305},
volume = {4},
year = {1992}
}
@book{Nielsen2015,
author = {Nielsen, Michael A.},
publisher = {Determination Press},
title = {{No Title}},
url = {http://neuralnetworksanddeeplearning.com/},
year = {2015}
}
@article{Deng2017,
abstract = {Vehicle detection in aerial images, being an interesting but challenging problem, plays an important role for a wide range of applications. Traditional methods are based on sliding-window search and handcrafted or shallow-learning-based features with heavy computational costs and limited representation power. Recently, deep learning algorithms, especially region-based convolutional neural networks (R-CNNs), have achieved state-of-the-art detection performance in computer vision. However, several challenges limit the applications of R-CNNs in vehicle detection from aerial images: 1) vehicles in large-scale aerial images are relatively small in size, and R-CNNs have poor localization performance with small objects; 2) R-CNNs are particularly designed for detecting the bounding box of the targets without extracting attributes; 3) manual annotation is generally expensive and the available manual annotation of vehicles for training R-CNNs are not sufficient in number. To address these problems, this paper proposes a fast and accurate vehicle detection framework. On one hand, to accurately extract vehicle-like targets, we developed an accurate-vehicle-proposal-network (AVPN) based on hyper feature map which combines hierarchical feature maps that are more accurate for small object detection. On the other hand, we propose a coupled R-CNN method, which combines an AVPN and a vehicle attribute learning network to extract the vehicle's location and attributes simultaneously. For original large-scale aerial images with limited manual annotations, we use cropped image blocks for training with data augmentation to avoid overfitting. Comprehensive evaluations on the public Munich vehicle dataset and the collected vehicle dataset demonstrate the accuracy and effectiveness of the proposed method.},
author = {Deng, Zhipeng and Sun, Hao and Zhou, Shilin and Zhao, Juanping and Zou, Huanxin},
doi = {10.1109/JSTARS.2017.2694890},
file = {:opt/pdf/07921594.pdf:pdf},
isbn = {9781509048472},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Attribute learning,convolutional neural networks,vehicle detection,vehicle proposal network},
number = {8},
pages = {3652--3664},
title = {{Toward Fast and Accurate Vehicle Detection in Aerial Images Using Coupled Region-Based Convolutional Neural Networks}},
volume = {10},
year = {2017}
}
@misc{Geman1992,
abstract = {Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'{e}}},
booktitle = {Neural Computation},
doi = {10.1162/neco.1992.4.1.1},
eprint = {arXiv:1011.1669v3},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geman, Bienenstock, Doursat - 1992 - Neural Networks and the BiasVariance Dilemma.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
number = {1},
pages = {1--58},
pmid = {24439530},
title = {{Neural Networks and the Bias/Variance Dilemma}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1992.4.1.1},
volume = {4},
year = {1992}
}
@article{Belharbi2017,
abstract = {A deep neural network model is a powerful framework for learning representations. Usually, it is used to learn the relation x → y by exploiting the regularities in the input x. In structured output prediction problems, y is multi-dimensional and structural relations often exist between the dimensions. The motivation of this work is to learn the output dependencies that may lie in the output data in order to improve the prediction accuracy. Unfortunately, feedforward networks are unable to exploit the relations between the outputs. In order to overcome this issue, we propose in this paper a regularization scheme for training neural networks for these particular tasks using a multi-task framework. Our scheme aims at incorporating the learning of the output representation y in the training process in an unsupervised fashion while learning the supervised mapping function x → y.We evaluate our framework on a facial landmark detection problem which is a typical structured output task. We show over two public challenging datasets (LFPW and HELEN) that our regularization scheme improves the generalization of deep neural networks and accelerates their training. The use of unlabeled data and label-only data is also explored, showing an additional improvement of the results. We provide an opensource implementation of our framework.},
archivePrefix = {arXiv},
arxivId = {1504.07550},
author = {Belharbi, Soufiane and H{\'{e}}rault, Romain and Chatelain, Cl{\'{e}}ment and Adam, S{\'{e}}bastien},
doi = {10.1016/j.neucom.2017.12.002},
eprint = {1504.07550},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Belharbi et al. - 2017 - Deep neural networks regularization for structured output prediction.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep learning,Multi-task learning,Regularization,Representations learning,Structured output prediction},
pages = {169--177},
publisher = {Elsevier B.V.},
title = {{Deep neural networks regularization for structured output prediction}},
url = {https://doi.org/10.1016/j.neucom.2017.12.002},
volume = {281},
year = {2017}
}
@article{Zeiler,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3557v1},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {arXiv:1301.3557v1},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - Unknown - Stochastic Pooling for Regularization of Deep Convolutional Neural Networks arXiv 1301 . 3557v1 cs . LG 16.pdf:pdf},
pages = {1--9},
title = {{Stochastic Pooling for Regularization of Deep Convolutional Neural Networks arXiv : 1301 . 3557v1 [ cs . LG ] 16 Jan 2013}}
}
@article{Boureau2009,
author = {Boureau, Y-lan},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boureau - 2009 - A Theoretical Analysis of Feature Pooling in Visual Recognition.pdf:pdf},
title = {{A Theoretical Analysis of Feature Pooling in Visual Recognition}},
year = {2009}
}
@article{Han,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.08630v2},
author = {Han, Shizhong and Meng, Zibo and Li, Zhiyuan and Reilly, James O and Cai, Jie and Wang, Xiaofeng and Tong, Yan},
eprint = {arXiv:1707.08630v2},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - Unknown - Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition.pdf:pdf},
title = {{Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition}}
}
@article{Iandola2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.07360v4},
author = {Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
eprint = {arXiv:1602.07360v4},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iandola et al. - 2017 - 50 X FEWER PARAMETERS AND 0 . 5MB MODEL SIZE.pdf:pdf},
pages = {1--13},
title = {{50 X FEWER PARAMETERS AND {\textless} 0 . 5MB MODEL SIZE}},
year = {2017}
}
@misc{Intrigano,
author = {Intrigano},
title = {{Neural Networks - Inception Network Motivation - YouTube}},
url = {https://www.youtube.com/watch?v=1OCsi0krPgg}
}
@article{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus Robert},
doi = {10.1007/978-3-642-35289-8-3},
eprint = {9809069v1},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 2012 - Efficient backprop.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {9--48},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Efficient backprop}},
volume = {7700 LECTU},
year = {2012}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
doi = {10.1109/ASRU.2015.7404828},
eprint = {1312.4400},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Chen, Yan - 2013 - Network In Network.pdf:pdf},
isbn = {9781479972913},
issn = {03029743},
pages = {1--10},
pmid = {24356345},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Li2017,
abstract = {Detecting small objects is notoriously challenging due to their low resolution and noisy representation. Existing object detection pipelines usually detect small objects through learning representations of all the objects at multiple scales. However, the performance gain of such ad hoc architectures is usually limited to pay off the computational cost. In this work, we address the small object detection problem by developing a single architecture that internally lifts representations of small objects to "super-resolved" ones, achieving similar characteristics as large objects and thus more discriminative for detection. For this purpose, we propose a new Perceptual Generative Adversarial Network (Perceptual GAN) model that improves small object detection through narrowing representation difference of small objects from the large ones. Specifically, its generator learns to transfer perceived poor representations of the small objects to super-resolved ones that are similar enough to real large objects to fool a competing discriminator. Meanwhile its discriminator competes with the generator to identify the generated representation and imposes an additional perceptual requirement - generated representations of small objects must be beneficial for detection purpose - on the generator. Extensive evaluations on the challenging Tsinghua-Tencent 100K and the Caltech benchmark well demonstrate the superiority of Perceptual GAN in detecting small objects, including traffic signs and pedestrians, over well-established state-of-the-arts.},
archivePrefix = {arXiv},
arxivId = {1706.05274},
author = {Li, Jianan and Liang, Xiaodan and Wei, Yunchao and Xu, Tingfa and Feng, Jiashi and Yan, Shuicheng},
doi = {10.1109/CVPR.2017.211},
eprint = {1706.05274},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Perceptual Generative Adversarial Networks for Small Object Detection.pdf:pdf},
issn = {1063-6919},
pages = {1951--1959},
title = {{Perceptual Generative Adversarial Networks for Small Object Detection}},
url = {http://arxiv.org/abs/1706.05274},
year = {2017}
}
@article{Hosseini-asl2016,
author = {Hosseini-asl, Ehsan},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosseini-asl - 2016 - ThinkIR The University of Louisville ' s Institutional Repository Sparse feature learning for image analysis in s.pdf:pdf},
keywords = {deep learning,disease diagnosis,feature learning,image analysis,machine learning,segmentation},
title = {{ThinkIR : The University of Louisville ' s Institutional Repository Sparse feature learning for image analysis in segmentation , classification , and disease diagnosis .}},
year = {2016}
}
@book{IanGoodfellow2016,
author = {{Ian Goodfellow} and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{rottensteiner2012isprs,
author = {Rottensteiner, Franz and Sohn, Gunho and Jung, Jaewook and Gerke, Markus and Baillard, Caroline and Benitez, Sebastien and Breitkopf, Uwe},
journal = {ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci},
number = {3},
pages = {293--298},
title = {{The ISPRS benchmark on urban object classification and 3D building reconstruction}},
volume = {1},
year = {2012}
}
@article{razakarivony2016vehicle,
abstract = {This paper introduces VEDAI: Vehicle Detection in Aerial Imagery a new database of aerial images provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit different variabilities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by different people can be properly reproduce and compared. Finally, the paper also gives the performance of baseline algorithms on this dataset, for different settings of these algorithms, to illustrate the difficulties of the task and provide baseline comparisons.},
author = {Razakarivony, S{\'{e}}bastien and Jurie, Frederic},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Razakarivony, Jurie - 2015 - Vehicle Detection in Aerial Imagery A Small Target Detection Benchmark.pdf:pdf},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Aerial imagery,Computer vision,Database,Detection,Infrared imagery,Low resolution images,Target detection,Vehicles},
pages = {187--203},
publisher = {Elsevier},
title = {{Vehicle Detection in Aerial Imagery : A Small Target Detection Benchmark}},
volume = {34},
year = {2015}
}
@book{ComaniciuD.2002,
author = {{Comaniciu D.} and P., MeerD.},
booktitle = {PAMI},
pages = {14.2},
title = {{Mean Shift: A Robust Approach toward Feature Space Analysis}},
year = {2002}
}
@article{Badrinarayanan2017,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
file = {:opt/pdf/1511.00561.pdf:pdf},
isbn = {9783319464879},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deep convolutional neural networks,decoder,encoder,indoor scenes,pooling,road scenes,semantic pixel-wise segmentation,upsampling},
number = {12},
pages = {2481--2495},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
volume = {39},
year = {2017}
}
@article{Sep2016,
abstract = {— An accurate and reliable image based fruit detec-tion system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. This paper presents the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Abla-tion studies are presented to better understand the practical deployment of the detection network, including how much training data is required to capture variability in the dataset. Data augmentation techniques are shown to yield significant performance gains, resulting in a greater than two-fold reduc-tion in the number of training images required. In contrast, transferring knowledge between orchards contributed to negli-gible performance gain over initialising the Deep Convolutional Neural Network directly from ImageNet features. Finally, to operate over orchard data containing between 100-1000 fruit per image, a tiling approach is introduced for the Faster R-CNN framework. The study has resulted in the best yet detection performance for these orchards relative to previous works, with an F1-score of {\textgreater} 0.9 achieved for apples and mangoes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.03677v2},
author = {Sep, R O},
eprint = {arXiv:1610.03677v2},
file = {:opt/pdf/07989417.pdf:pdf},
isbn = {9781509046331},
pages = {3626--3633},
title = {{Deep Fruit Detection in Orchards}},
year = {2016}
}
@article{kembhavi2011vehicle,
author = {Kembhavi, Aniruddha and Harwood, David and Davis, Larry S},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
pages = {1250--1265},
publisher = {IEEE},
title = {{Vehicle detection using partial least squares}},
volume = {33},
year = {2011}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sulskever, IIya and Hinton, Geoffrey E Geoffret E and Sutskever, Ilya and Hinton, Geoffrey E Geoffret E},
doi = {10.1145/3065386},
eprint = {1102.0183},
file = {:opt/pdf/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf;:opt/pdf/alexnet{\_}tugce{\_}kyunghee.pdf:pdf;:opt/pdf/ImageNet classification with deep convolutional neural networks.pdf:pdf},
isbn = {9781627480031},
issn = {00010782},
journal = {Advances in Neural Information and Processing Systems (NIPS)},
number = {6},
pages = {84--90},
pmid = {1214229},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://doi.org/10.1145/3065386},
volume = {60},
year = {2012}
}
@article{liu2011automated,
author = {Liu, Wen and Yamazaki, Fumio and Vu, Tuong Thuy},
journal = {IEEE Journal of selected topics in applied earth observations and remote sensing},
number = {1},
pages = {75},
title = {{Automated vehicle extraction and speed determination from QuickBird satellite images}},
volume = {4},
year = {2011}
}
@article{article,
author = {Hinz, Stefan and Stilla, Uwe and Leitloff, Jens},
journal = {Geoscience and Remote Sensing, IEEE Transactions on},
pages = {2795--2806},
title = {{Vehicle Detection in Very High Resolution Satellite Images of City Areas}},
volume = {48},
year = {2010}
}
@article{salehi2012automatic,
author = {Salehi, Bahram and Zhang, Yun and Zhong, Ming},
journal = {IEEE Journal of selected topics in applied earth observations and remote sensing},
number = {1},
pages = {135--145},
publisher = {IEEE},
title = {{Automatic moving vehicles information extraction from single-pass WorldView-2 imagery}},
volume = {5},
year = {2012}
}
@article{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:opt/pdf/1505.04597.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
pages = {1--8},
pmid = {23285570},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
year = {2015}
}
@article{Lin2017,
abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
archivePrefix = {arXiv},
arxivId = {1708.02002},
author = {Lin, Tsung Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
doi = {10.1109/ICCV.2017.324},
eprint = {1708.02002},
file = {:opt/pdf/08237586.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2999--3007},
pmid = {23766329},
title = {{Focal Loss for Dense Object Detection}},
volume = {2017-Octob},
year = {2017}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
doi = {10.1109/CVPR.2015.7299176},
eprint = {1312.6229},
file = {:opt/pdf/1312.6229.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
pmid = {1000200972},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{Lin2017a,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1612.03144},
author = {Lin, Tsung Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
doi = {10.1109/CVPR.2017.106},
eprint = {1612.03144},
file = {:opt/pdf/08099589.pdf:pdf},
isbn = {9781538604571},
issn = {0006-291X},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {936--944},
pmid = {303902},
title = {{Feature pyramid networks for object detection}},
volume = {2017-Janua},
year = {2017}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:opt/pdf/1512.00567.pdf:pdf;:opt/pdf/Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:opt/pdf/1311.2524.pdf:pdf;:opt/pdf/06909475.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
doi = {10.1109/ICCV.2017.322},
eprint = {1703.06870},
file = {:opt/pdf/1703.06870.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2980--2988},
pmid = {303902},
title = {{Mask R-CNN}},
volume = {2017-Octob},
year = {2017}
}
@article{Chen2014,
abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6{\%} IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
archivePrefix = {arXiv},
arxivId = {1412.7062},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1412.7062},
file = {:opt/pdf/07913730.pdf:pdf},
isbn = {9783901608353},
issn = {01628828},
number = {4},
pages = {834--848},
pmid = {28463186},
title = {{Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1412.7062},
volume = {40},
year = {2014}
}
@article{Zhu2016,
abstract = {Although promising results have been achieved in the ar-eas of traffic-sign detection and classification, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. It provides 100000 im-ages containing 30000 traffic-sign instances. These images cover large variations in illuminance and weather condi-tions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. We call this benchmark Tsinghua-Tencent 100K. Secondly, we demon-strate how a robust end-to-end convolutional neural net-work (CNN) can simultaneously detect and classify traffic-signs. Most previous CNN image processing solutions tar-get objects that occupy a large proportion of an image, and such networks do not work well for target objects occupy-ing only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our net-work and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available 1 .},
author = {Zhu, Zhe and Liang, Dun and Zhang, Songhai and Huang, Xiaolei and Li, Baoli and Hu, Shimin},
doi = {10.1109/CVPR.2016.232},
file = {:home/mike/Downloads/Zhu{\_}Traffic-Sign{\_}Detection{\_}and{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2110--2118},
title = {{Traffic-Sign Detection and Classification in the Wild}},
url = {http://ieeexplore.ieee.org/document/7780601/},
year = {2016}
}
@article{Zhou2015,
abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1{\%} top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2{\%} top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
archivePrefix = {arXiv},
arxivId = {1512.04150},
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
doi = {10.1109/CVPR.2016.319},
eprint = {1512.04150},
file = {:home/mike/Downloads/1512.04150.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {0920-5691},
pmid = {314800},
title = {{Learning Deep Features for Discriminative Localization}},
url = {http://arxiv.org/abs/1512.04150},
year = {2015}
}
@article{Redmon2017,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
file = {:home/mike/Downloads/1612.08242.pdf:pdf},
isbn = {9781538604571},
issn = {0146-4833},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {6517--6525},
pmid = {23021419},
title = {{YOLO9000: Better, faster, stronger}},
volume = {2017-Janua},
year = {2017}
}
@article{Huang2017,
abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
archivePrefix = {arXiv},
arxivId = {1611.10012},
author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
doi = {10.1109/CVPR.2017.351},
eprint = {1611.10012},
file = {:home/mike/Downloads/1611.10012.pdf:pdf},
isbn = {9781538604571},
issn = {16113349},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {3296--3305},
pmid = {23739795},
title = {{Speed/accuracy trade-offs for modern convolutional object detectors}},
volume = {2017-Janua},
year = {2017}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
doi = {arXiv:1207.0580},
eprint = {1207.0580},
file = {:opt/pdf/Improving neural networks by preventing.pdf:pdf},
isbn = {9781467394673},
issn = {9781467394673},
pages = {1--18},
pmid = {1000104337},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Sevo2016,
abstract = {We are witnessing daily acquisition of large amounts of aerial and satellite imagery. Analysis of such large quantities of data can be helpful for many practical applications. In this letter, we present an automatic content-based analysis of aerial imagery in order to detect and mark arbitrary objects or regions in high-resolution images. For that purpose, we proposed a method for automatic object detection based on a convolutional neural network. A novel two-stage approach for network training is implemented and verified in the tasks of aerial image classification and object detection. First, we tested the proposed training approach using UCMerced data set of aerial images and achieved accuracy of approximately 98.6{\%}. Second, the method for automatic object detection was implemented and verified. For implementation on GPGPU, a required processing time for one aerial image of size 5000 �� 5000 pixels was around 30 s.},
archivePrefix = {arXiv},
arxivId = {1512.05227},
author = {Sevo, Igor and Avramovic, Aleksej},
doi = {10.1109/LGRS.2016.2542358},
eprint = {1512.05227},
file = {:opt/pdf/CNNODAI.pdf:pdf},
isbn = {9781467388511},
issn = {1545-598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {graphics processing units,image classification,neu},
number = {5},
pages = {740--744},
title = {{Convolutional Neural Network Based Automatic Object Detection on Aerial Images}},
url = {http://arxiv.org/abs/1505.04597},
volume = {13},
year = {2016}
}
@article{Tran2015,
abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8{\%} accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
doi = {10.1109/ICCV.2015.510},
eprint = {1412.0767},
file = {:opt/pdf/Learning Spatiotemporal Features with 3D Convolutional Networks.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {4489--4497},
pmid = {22392705},
title = {{Learning spatiotemporal features with 3D convolutional networks}},
volume = {2015 Inter},
year = {2015}
}
@article{Zhao2016,
abstract = {It is widely agreed that spatial features can be combined with spectral properties for improving interpretation performances on very-high-resolution (VHR) images in urban areas. However, many existing methods for extracting spatial features can only generate low-level features and consider limited scales, leading to unpleasant classification results. In this study, multiscale convolutional neural network (MCNN) algorithm was presented to learn spatial-related deep features for hyperspectral remote imagery classification. Unlike traditional methods for extracting spatial features, the MCNN first transforms the original data sets into a pyramid structure containing spatial information at multiple scales, and then automatically extracts high-level spatial features using multiscale training data sets. Specifically, the MCNN has two merits: (1) high-level spatial features can be effectively learned by using the hierarchical learning structure and (2) multiscale learning scheme can capture contextual information at different scales. To evaluate the effectiveness of the proposed approach, the MCNN was applied to classify the well-known hyperspectral data sets and compared with traditional methods. The experimental results shown a significant increase in classification accuracies especially for urban areas.},
author = {Zhao, Wenzhi and Du, Shihong},
doi = {10.1016/j.isprsjprs.2016.01.004},
file = {:opt/pdf/Learning multiscale and deep representations for classifying remotelysensed imagery.pdf:pdf},
isbn = {0924-2716},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Deep learning,Feature extraction,Multiscale convolutional neural network (MCNN),Remote sensing image classification},
pages = {155--165},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Learning multiscale and deep representations for classifying remotely sensed imagery}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2016.01.004},
volume = {113},
year = {2016}
}
@article{Kellenberger2017,
abstract = {Illegal wildlife poaching poses one severe threat to the en-vironment. Measures to stem poaching have only been with limited success, mainly due to efforts required to keep track of wildlife stock and animal tracking. Recent developments in remote sensing have led to low-cost Unmanned Aerial Ve-hicles (UAVs), facilitating quick and repeated image acqui-sitions over vast areas. In parallel, progress in object detec-tion in computer vision yielded unprecedented performance improvements, partially attributable to algorithms like Con-volutional Neural Networks (CNNs). We present an object detection method tailored to detect large animals in UAV im-ages. We achieve a substantial increase in precision over a robust state-of-the-art model on a dataset acquired over the Kuzikus wildlife reserve park in Namibia. Furthermore, our model processes data at over 72 images per second, as op-posed 3 for the baseline, allowing for real-time applications.},
author = {Kellenberger, Benjamin and Volpi, Michele and Tuia, Devis},
doi = {10.1109/IGARSS.2017.8127090},
file = {:opt/pdf/fast animal detection.pdf:pdf},
isbn = {9781509049516},
journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
pages = {866--869},
title = {{Fast animal detection in UAV images using convolutional neural networks}},
volume = {2017-July},
year = {2017}
}
@article{Biolchi2016,
abstract = {{\textcopyright} 2014 Sara Biolchi. This paper presents the outcomes of a geomorphological investigation carried out along the coasts of the island of Malta and provides a detailed classification of the Maltese coastline based on the identification and definition of specific coastal geomorphotypes. The results of field surveys, supported by air-photo interpretation, have led to the production of a coastal geomorphological map at 1:30,000 scale which outlines the processes and related deposits and landforms. The latter are the result of the complex interplay of structural, gravitational, coastal and karst processes. Moreover, radiocarbon dates of marine organisms encrusted on boulders mapped along the NE coast are presented.},
author = {Biolchi, Sara and Furlani, Stefano and Devoto, Stefano and Gauci, Ritienne and Castaldini, Doriano and Soldati, Mauro},
doi = {10.1080/17445647.2014.984001},
file = {:opt/pdf/Geomorphological identification classification and spatial distribution of coastal landforms of Malta Mediterranean Sea.pdf:pdf},
issn = {17445647},
journal = {Journal of Maps},
keywords = {Coastal geomorphology,Coastal landforms,Geomorphotypes,Malta},
number = {1},
pages = {87--99},
publisher = {Taylor {\&} Francis},
title = {{Geomorphological identification, classification and spatial distribution of coastal landforms of Malta (Mediterranean Sea)}},
volume = {12},
year = {2016}
}
@article{Ammour2017,
abstract = {This paper presents an automatic solution to the problem of detecting and counting cars in unmanned aerial vehicle (UAV) images. This is a challenging task given the very high spatial resolution of UAV images (on the order of a few centimetres) and the extremely high level of detail, which require suitable automatic analysis methods. Our proposed method begins by segmenting the input image into small homogeneous regions, which can be used as candidate locations for car detection. Next, a window is extracted around each region, and deep learning is used to mine highly descriptive features from these windows. We use a deep convolutional neural network (CNN) system that is already pre-trained on huge auxiliary data as a feature extraction tool, combined with a linear support vector machine (SVM) classifier to classify regions into “car” and “no-car” classes. The final step is devoted to a fine-tuning procedure which performs morphological dilation to smooth the detected regions and fill any holes. In addition, small isolated regions are analysed further using a few sliding rectangular windows to locate cars more accurately and remove false positives. To evaluate our method, experiments were conducted on a challenging set of real UAV images acquired over an urban area. The experimental results have proven that the proposed method outperforms the state-of-the-art methods, both in terms of accuracy and computational time.},
author = {Ammour, Nassim and Alhichri, Haikel and Bazi, Yakoub and Benjdira, Bilel and Alajlan, Naif and Zuair, Mansour},
doi = {10.3390/rs9040312},
file = {:opt/pdf/Deep Learning Approach for Car Detection.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Car counting,Convolutional neural networks (CNNs),Deep learning,Mean-shift segmentation,Support vector machines (SVM),UAV imagery},
number = {4},
title = {{Deep learning approach for car detection in UAV imagery}},
volume = {9},
year = {2017}
}
@article{Giusti2013,
archivePrefix = {arXiv},
arxivId = {1302.1700},
author = {Giusti, Alessandro and Cire, Dan C and Masci, Jonathan and Gambardella, Luca M},
eprint = {1302.1700},
file = {:opt/pdf/Fast image scanning with deep max-pooling convolutional neural networks.pdf:pdf},
isbn = {9781479923410},
pages = {4034--4038},
title = {{Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks san urgen Schmidhuber Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks san}},
year = {2013}
}
@article{Qayyum2017,
abstract = {ABSTRACTAerial scene classification purposes to automatically label aerial images with specific semantic categories. However, cataloguing presents a fundamental problem for high-resolution remote-sensing imagery (HRRS). Recent developments include several approaches and numerous algorithms address the task. This article proposes a convolutional neural network (CNN) approach that utilizes sparse coding for scene classification applicable for HRRS unmanned aerial vehicle (UAV) and satellite imagery. The article has two major sections: the first describes the extraction of dense multiscale features (multiple scales) from the last convolutional layer of a pre-trained CNN models; the second describes the encoding of extracted features into global image features via sparse coding to achieve scene classification. The authors compared experimental outcomes with existing techniques such as Scale-Invariant Feature Transform and demonstrated that features from pre-trained CNNs generalized well with HRRS datasets and were more expressive than low- and mid-level features, exhibiting an overall 90.3{\%} accuracy rate for scene classification compared to 85.4{\%} achieved by SIFT with sparse coding. Thus, the proposed CNN-based sparse coding approach obtained a robust performance that holds promising potential for future applications in satellite and UAV imaging.},
author = {Qayyum, Abdul and Malik, Aamir Saeed and Saad, Naufal M. and Iqbal, Mahboob and {Faris Abdullah}, Mohd and Rasheed, Waqas and {Rashid Abdullah}, Tuan AB and {Bin Jafaar}, Mohd Yaqoob},
doi = {10.1080/01431161.2017.1296206},
file = {:opt/pdf/Scene classification for aerial images based on CNN using sparse coding technique.pdf:pdf},
issn = {13665901},
journal = {International Journal of Remote Sensing},
number = {8-10},
pages = {2662--2685},
publisher = {Taylor {\&} Francis},
title = {{Scene classification for aerial images based on CNN using sparse coding technique}},
url = {http://dx.doi.org/10.1080/01431161.2017.1296206},
volume = {38},
year = {2017}
}
@article{Zhong2017a,
author = {Zhong, Jiandan and Lei, Tao and Yao, Guangle},
doi = {10.3390/s17122720},
file = {:opt/pdf/Robust Vehicle Detection in Aerial Images Based onCascaded Convolutional Neural Networks.pdf:pdf},
isbn = {8613880495},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Aerial image,Convolutional neural network,Deep learning,Vehicle detection},
number = {12},
title = {{Robust vehicle detection in aerial images based on cascaded convolutional neural networks}},
volume = {17},
year = {2017}
}
@article{Audebert2017,
abstract = {Like computer vision before, remote sensing has been radically changed by the introduction of deep learning and, more notably, Convolution Neural Networks. Land cover classification, object detection and scene understanding in aerial images rely more and more on deep networks to achieve new state-of-the-art results. Recent architectures such as Fully Convolutional Networks can even produce pixel level annotations for semantic mapping. In this work, we present a deep-learning based segment-before-detect method for segmentation and subsequent detection and classification of several varieties of wheeled vehicles in high resolution remote sensing images. This allows us to investigate object detection and classification on a complex dataset made up of visually similar classes, and to demonstrate the relevance of such a subclass modeling approach. Especially, we want to show that deep learning is also suitable for object-oriented analysis of Earth Observation data as effective object detection can be obtained as a byproduct of accurate semantic segmentation. First, we train a deep fully convolutional network on the ISPRS Potsdam and the NZAM/ONERA Christchurch datasets and show how the learnt semantic maps can be used to extract precise segmentation of vehicles. Then, we show that those maps are accurate enough to perform vehicle detection by simple connected component extraction. This allows us to study the repartition of vehicles in the city. Finally, we train a Convolutional Neural Network to perform vehicle classification on the VEDAI dataset, and transfer its knowledge to classify the individual vehicle instances that we detected.},
author = {Audebert, Nicolas and {Le Saux}, Bertrand and Lef{\`{e}}vre, S{\'{e}}bastien},
doi = {10.3390/rs9040368},
file = {:opt/pdf/Vehicle Detection andClassification through Semantic Segmentation ofAerial Images.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Deep learning,Object classification,Semantic segmentation,Vehicle detection},
number = {4},
title = {{Segment-before-detect: Vehicle detection and classification through semantic segmentation of aerial images}},
volume = {9},
year = {2017}
}
@article{Basaeed2016,
abstract = {In this paper, a region segmentation technique for remote sensing images using a boosted committee of Convolutional Neural Networks (CNNs) coupled with inter-band and intra-band fusion, is proposed. The vast heterogeneity in remote sensing images restricts the application of existing segmentation methods that often rely on a set of predefined feature detectors along with tunable parameters. Therefore, it is highly challenging to design a segmentation technique which could achieve high accuracy while simultaneously maintaining strong generalization particularly for visual data with improved spatial, spectral, and temporal resolutions. The proposed method is a fusion framework consisting of a set of thirty boosted networks that derive individual probability maps on the location of region boundaries from the different multi-spectral bands and combines them into one using an averaging inter-band fusion scheme. The boundaries are then thinned, connected, and region segmented using a morphological intra-band fusion scheme. Qualitative and quantitative results, on publicly-available datasets, confirm the superiority of the proposed segmentation method over existing state-of-art techniques. In addition, the paper also demonstrates the effect of some variations in design-choices of the proposed method.},
author = {Basaeed, Essa and Bhaskar, Harish and Al-Mualla, Mohammed},
doi = {10.1016/j.knosys.2016.01.028},
file = {:opt/pdf/Supervised remote sensing image segmentation using boostedconvolutional neural networks.pdf:pdf},
isbn = {09507051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Artificial neural networks,Image segmentation,Multispectral imaging,Remote sensing},
pages = {19--27},
publisher = {Elsevier B.V.},
title = {{Supervised remote sensing image segmentation using boosted convolutional neural networks}},
url = {http://dx.doi.org/10.1016/j.knosys.2016.01.028},
volume = {99},
year = {2016}
}
@article{Dp,
author = {Dp, O D D Q G and Dffxudf, Vkrz and Lpdjh, I R U and Dw, Odqgvfdsh and Sduwlfxodu, D Q and Wkhq, Orfdwlrq and Dowlwxgh, D Orzhu},
file = {:opt/pdf/realtime scene understanding for UAV.pdf:pdf},
pages = {5--8},
title = {{Real ­ time Scene Understanding for UAV Imagery based on Deep Convolutional Neural Networks}}
}
@article{Zeggada2016,
abstract = {In this paper, we present a multilabel classification method for images acquired by means of Unmanned Ariel Vehicles (UAV) over urban areas. Due to the fact that UAV-grabbed images are characterized by extremely high spatial resolution, usual recognition schemes (such as traditional satellite or airborne based images) are likely to fail. In this work, tile- based multilabel classification framework is adopted to overcome such issue. In particular, a given UAV-shot image is first subdivided into a grid of equal tiles. Next, deep neural network-induced features are extracted from each tile and then fed into a radial basis function neural network classifier in order to infer the corresponding object list. We apply a refinement step at the top of the complete deep network architecture to boost the classification results. The proposed method was evaluated on a dataset acquired over the city of Trento, Italy with an hexacopter UAV. Superior classification rates have been scored with respect to the state-of-the-art.},
author = {Zeggada, Abdallah and Melgani, Farid},
doi = {10.1109/IGARSS.2016.7730325},
file = {:opt/pdf/Multilabel classification of UAV images with Convolutional Neural Networks.pdf:pdf},
isbn = {9781509033324},
journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
keywords = {coarse description,extremely high resolution,image analysis,image multilabeling,multi-object detection,unmanned aerial vehicle},
pages = {5083--5086},
title = {{Multilabel classification of UAV images with Convolutional Neural Networks}},
volume = {2016-Novem},
year = {2016}
}
@article{Meng2017,
abstract = {In the past decade, Convolutional Neural Networks (CNNs) have been demonstrated successful for object detections. However, the size of network input is limited by the amount of memory available on GPUs. Moreover, performance degrades when detecting small objects. To alleviate the memory usage and improve the performance of detecting small traffic signs, we proposed an approach for detecting small traffic signs from large images under real world conditions. In particular, large images are broken into small patches as input to a Small-Object-Sensitive-CNN (SOS-CNN) modified from a Single Shot Multibox Detector (SSD) framework with a VGG-16 network as the base network to produce patch-level object detection results. Scale invariance is achieved by applying the SOS-CNN on an image pyramid. Then, image-level object detection is obtained by projecting all the patch-level detection results to the image at the original scale. Experimental results on a real-world conditioned traffic sign dataset have demonstrated the effectiveness of the proposed method in terms of detection accuracy and recall, especially for those with small sizes.},
archivePrefix = {arXiv},
arxivId = {1706.08574},
author = {Meng, Zibo and Fan, Xiaochuan and Chen, Xin and Chen, Min and Tong, Yan},
doi = {10.1109/IRI.2017.57},
eprint = {1706.08574},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meng et al. - 2017 - Detecting small signs from large images.pdf:pdf},
isbn = {9781538615621},
journal = {Proceedings - 2017 IEEE International Conference on Information Reuse and Integration, IRI 2017},
pages = {217--224},
title = {{Detecting small signs from large images}},
volume = {2017-Janua},
year = {2017}
}
@book{Forsyth2012,
author = {Forsyth, David a. and {Jean Ponce}},
file = {:home/mike/Downloads/Computer Vision A Modern Approach 2nd Edition.pdf:pdf},
isbn = {9780136085928},
issn = {0218-0014},
pages = {761},
title = {{Computer Vision A Modern Approach 2nd Edition.pdf}},
year = {2012}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
file = {:opt/pdf/reluICML.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Madaan2017,
abstract = {Wire detection is a key capability for safe navigation of autonomous aerial vehicles and is a challenging problem as wires are generally only a few pixels wide, can appear at any orientation and location, and are hard to distinguish from other similar looking lines and edges. We leverage the recent advances in deep learning by treating wire detection as a semantic segmentation task, and investigate the effectiveness of convolutional neural networks for the same. To find an optimal model in terms of detection accuracy and real time performance on a portable GPU, we perform a grid search over a finite space of architectures. Further, to combat the issue of unavailability of a large public dataset with annotations, we render synthetic wires using a ray tracing engine, and overlay them on 67K images from flight videos available on the internet. We use this synthetic dataset for pretraining our models before finetuning on real data, and show that synthetic data alone can lead to pretty accurate detections qualitatively as well. We also verify if providing explicit information about local evidence of wiry-ness in the form of edge and line detection results from a traditional computer vision method, as additional channels to the network input, makes the task easier or not. We evaluate our best models from the grid search on a publicly available dataset and show that they outperform previous work using traditional computer vision and various deep net baselines of FCNs, SegNet and E-Net, on both standard edge detection metrics and inference speed. Our top models run at more than 3Hz on the NVIDIA Jetson TX2 with input resolution of 480×640, with an Average Precision score of 0.73 on our test split of the USF dataset.},
author = {Madaan, Ratnesh and Maturana, Daniel and Scherer, Sebastian},
doi = {10.1109/IROS.2017.8206190},
file = {:opt/pdf/wire detection synthetic data DCNN UAV.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {International Conference on Intelligent Robots and Systems},
keywords = {Aerial Systems: Perception and Autonomy,Deep Learn},
pages = {3487--3494},
title = {{Wire Detection using Synthetic Data and Dilated Convolutional Networks for Unmanned Aerial Vehicles}},
year = {2017}
}
@article{MinistryforSustainableDevelopmenttheEnvironmentandClimateChange2014,
abstract = {Foreword It is my pleasure to be able to present Malta's Waste Management Plan for the Maltese Islands which covers the period up till 2020. We have purposely given this Plan a resource management approach for we firmly believe that waste is increasingly becoming a resource from which we do not only derive recycled materials, that lengthen the life cycle of virgin resources, or embedded energy but also a greener economy and the creation of more green jobs in line with the architecture of modern economies. I am proud at what we have been able to achieve in such a short time. Upon coming into office we found that we had a very tight deadline by when to submit Malta's Waste Management Plan and Waste Prevention Plan. I was firmly of the belief that success in this sector can only be achieved if society were to make the required commitment. To this effect the authors of this Plan agreed to undertake as wide a consultation exercise as possible during which we met interested stakeholders, initially, without any preconceived ideas and, later, put out a draft Plan for further consultation in parallel with an SEA which was being conducted pari passu with the development of this Plan. On paper, I am confident that the Plan will find the approval of a wide majority of society whatever walk of life they come from. However, the proof of the pudding is in the eating and this Plan's success on the ground can only be achieved if every member of society assumes his and her responsibility in committing to the national waste management agenda. What we are proposing is not any different from success stories in other fellow European countries and which have come about through multi-stakeholder commitments. Malta is a very small country where the impacts of environmental problems can be felt throughout the islands. Therefore no one can be detached from this problem. It is now time to turn this challenge into an opportunity. Better waste management practices, including minimizing our waste, will undoubtedly re-size the problem we are faced with, require less of an infrastructure and hence have a lower impact on the environment. An out of sight, out of mind approach can only lead to a more expensive waste management system and one where the cost of inaction is high. We have witnessed the concerns of those who live in the proximity of sites designated for waste management facilities. The consequence of inaction will mean that more of our limited land areas will have to be dedicated to such.},
author = {{Ministry for Sustainable Development the Environment and Climate Change}},
file = {:opt/pdf/waste management plan 2014 - 2020 - final document.pdf:pdf},
number = {January},
pages = {1--209},
title = {{WASTE MANAGEMENT PLAN FOR THE MALTESE ISLANDS: A Resource Management Approach 2014-2020}},
year = {2014}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:opt/pdf/yolo.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {01689002},
pmid = {27295650},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Szegedy2015a,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and Hill, Chapel and Arbor, Ann},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:opt/pdf/1409.4842.pdf:pdf;:opt/pdf/Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{Alom2018TheApproaches,
abstract = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
archivePrefix = {arXiv},
arxivId = {1803.01164},
author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westberg, Stefan and Hasan, Mahmudul and {Van Esesn}, Brian C and Awwal, Abdul A S. and Asari, Vijayan K.},
eprint = {1803.01164},
file = {:opt/pdf/1803.01164.pdf:pdf},
journal = {arXiv preprint arXiv:1803.01164},
title = {{The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches}},
url = {http://arxiv.org/abs/1803.01164},
year = {2018}
}
@article{David2014,
author = {David, S},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/David - 2014 - a Comparative Study of Various.pdf:pdf},
isbn = {9781538618875},
journal = {2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)},
keywords = {code,dct,decimal matrix code,dmc,dwt,eccs,error correction codes,harr wavelets,inverse radon transform,lifting schemes,matrix,mcs,mcus,multiple cell upsets,pds,punctured deference set,radon transform,reed muller code,rmc,sphit},
number = {4},
pages = {16--20},
publisher = {IEEE},
title = {{a Comparative Study of Various Image}},
volume = {3},
year = {2014}
}
@article{Ren2017,
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:opt/pdf/07485869(1).pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with}},
url = {https://github.com/rbgirshick/py-faster-rcnn{\%}0Ahttps://github.com/shaoqingren/faster{\_}rcnn},
volume = {39},
year = {2017}
}
@article{Guan2017,
author = {Guan, Tongfan and Zhu, Hao},
doi = {10.1109/ICMIP.2017.37},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guan, Zhu - 2017 - Atrous Faster R-CNN for Small Scale Object Detection.pdf:pdf},
isbn = {9781509059546},
journal = {Proceedings - 2017 2nd International Conference on Multimedia and Image Processing, ICMIP 2017},
keywords = {atrous convolution,component,deep learnng,object detection,small scale},
pages = {16--21},
title = {{Atrous Faster R-CNN for Small Scale Object Detection}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{ChristianEggertStephanBrehmAntonWinschelDanZecha2017,
address = {University of Augsburg},
author = {{Christian Eggert , Stephan Brehm , Anton Winschel , Dan Zecha}, Rainer Lienhart},
booktitle = {Proceedings of the 2017 International Conference on Multimedia and Expo},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christian Eggert , Stephan Brehm , Anton Winschel , Dan Zecha - 2017 - A CLOSER LOOK SMALL OBJECT DETECTION IN FASTER R-CNN Multimedia.pdf:pdf},
isbn = {9781509060672},
title = {{A CLOSER LOOK : SMALL OBJECT DETECTION IN FASTER R-CNN Multimedia Computing and Computer Vision Lab}},
volume = {0},
year = {2017}
}
@book{Menikdiwela2017,
abstract = {{\textcopyright} 2017 IEEE. Object detection is a well-studied topic, however detection of small objects still lacks attention. Detecting small objects has been difficult due to small sizes, occlusion and complex backgrounds. Small objects detection is important in a number of applications including detection of small insects. One application is spider detection and removal. Spiders are frequently found on grapes and broccolis sold at supermarkets and this poses a significant safety issue and generates negative publicity for the industry. In this paper, we present a fine-tuned VGG16 network for detection of small objects such as spiders. Furthermore, we introduce a simple technique called 'feature activation mapping' for object visualization from VGG16 feature maps. The testing accuracy of our network on tiny spiders with various backgrounds is 84{\%}, as compared to 72{\%} using fined-tuned Faster R-CNN and 95.32{\%} using CAM. Even though our feature activation mapping technique has a mid-range of test accuracy, it provides more detailed shape and size of spiders than using CAM which is important for the application area. A data set for spider detection is made available online.},
author = {Menikdiwela, Medhani and Nguyen, Chuong and Li, Hongdong and Shaw, Marnie},
booktitle = {International Conference Image and Vision Computing New Zealand},
doi = {10.1109/IVCNZ.2017.8402455},
file = {:opt/pdf/08402455.pdf:pdf},
isbn = {9781538642764},
issn = {21512205},
keywords = {CNN,R-CNN,feature activation map,heat map,object detection},
number = {December},
pages = {1--5},
publisher = {IEEE},
title = {{CNN-based small object detection and visualization with feature activation mapping}},
volume = {2017-Decem},
year = {2018}
}
@article{Chen2018,
abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
archivePrefix = {arXiv},
arxivId = {1606.00915},
author = {Chen, Liang Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
doi = {10.1109/TPAMI.2017.2699184},
eprint = {1606.00915},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2018 - DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional neural networks,atrous convolution,conditional random fields,semantic segmentation},
number = {4},
pages = {834--848},
pmid = {28463186},
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
volume = {40},
year = {2018}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:opt/pdf/07410526.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{Tayara2017,
author = {Tayara, Hilal and Soo, Kim Gil and Chong, Kil To},
doi = {10.1109/ACCESS.2017.2782260},
file = {:opt/pdf/08186148.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Aerial images,convolution neural network (CNN),deep learning,regression,vehicle detection},
number = {c},
pages = {2220--2230},
title = {{Vehicle Detection and Counting in High-Resolution Aerial Images Using Convolutional Regression Neural Network}},
volume = {6},
year = {2017}
}
@article{Rezaee2018,
author = {Rezaee, Mohammad and Mahdianpari, Masoud and Zhang, Yun and Salehi, Bahram},
doi = {10.1109/JSTARS.2018.2846178},
file = {:opt/pdf/08401505.pdf:pdf},
issn = {1939-1404},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
pages = {1--10},
publisher = {IEEE},
title = {{Deep Convolutional Neural Network for Complex Wetland Classification Using Optical Remote Sensing Imagery}},
url = {https://ieeexplore.ieee.org/document/8401505/},
volume = {PP},
year = {2018}
}
@article{Ren2017a,
abstract = {Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them "Networks on Convolutional feature maps" (NoCs). We discover that aside from deep feature maps, a deep and convolutional per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly lead to good detection accuracy without using such a per-region classifier. We show by experiments that despite the effective ResNets and Faster R-CNN systems, the design of NoCs is an essential element for the 1st-place winning entries in ImageNet and MS COCO challenges 2015.},
archivePrefix = {arXiv},
arxivId = {1504.06066},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Zhang, Xiangyu and Sun, Jian},
doi = {10.1109/TPAMI.2016.2601099},
eprint = {1504.06066},
file = {:opt/pdf/07546875.pdf:pdf},
isbn = {9781467369640},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {CNN,Object detection,convolutional feature map},
number = {7},
pages = {1476--1481},
title = {{Object detection networks on convolutional feature maps}},
volume = {39},
year = {2017}
}
@article{Malof2016,
author = {Malof, Jordan M and Bradbury, Kyle and Collins, Leslie M. and Newell, Richard G.},
doi = {10.1109/ICRERA.2016.7884415},
file = {:opt/pdf/07884415.pdf:pdf},
isbn = {9781509033881},
journal = {International Conference on Renewable Energy Research and Applications (ICRERA)},
keywords = {-component,1hzhoo,5lfkdug,convolutional neural networks,deep,detection,energy,learning,oh,photovoltaic,qhuj,qlwldwlyh,solar,udgexu,xnh 8qlyhuvlw},
pages = {650--654},
title = {{A Deep Convolutional Neural Network and a Random Forest Classifier for Solar Photovoltaic Array Detection in Aerial Imagery}},
volume = {5},
year = {2016}
}
@article{Brahmbhatt2017,
abstract = {A quadrotor Micro Aerial Vehicle (MAV) is designed to navigate a track using neural network approach to identify the direction of the path from a stream of monocular images received from a downward-facing camera mounted on the vehicle. Current autonomous MAVs mainly employ computer vision techniques based on image processing and feature tracking for vision-based navigation tasks. It requires expensive onboard computation and can create latency in the real-time system when working with low-powered computers. By using a supervised image classifier, we shift the costly computational task of training a neural network to classify the direction of the track to an off-board computer. We make use of the learned weights obtained after training to perform simple mathematical operations to predict the class of the image on the onboard computer. We compare the computer vision based tracking approach with the proposed approach to navigate a track using a quadrotor and show that the processing rates of the latter is faster. This allows low-cost, low-powered computers such as the Raspberry Pi to be used efficiently as onboard companion computers for flying vision-based autonomous missions with MAVs.},
author = {Brahmbhatt, Khushal and Pai, Akshatha Rakesh and Singh, Sanjay},
doi = {10.1109/ICACCI.2017.8125902},
file = {:opt/pdf/--neural network approach for vision based track navigation.pdf:pdf},
isbn = {9781509063673},
journal = {2017 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2017},
keywords = {Autonomous navigation,MAVs,UAVs,Vision-based navigation},
pages = {578--583},
title = {{Neural network approach for vision-based track navigation using low-powered computers on MAVs}},
volume = {2017-Janua},
year = {2017}
}
@article{Sharma2017,
abstract = {Availability of accurate land cover information over large areas is essential to the global environment sustainability; digital classification using medium-resolution remote sensing data would provide an effective method to generate the required land cover information. However, low accuracy of existing per-pixel based classification methods for medium-resolution data is a fundamental limiting factor. While convolutional neural networks (CNNs) with deep layers have achieved unprecedented improvements in object recognition applications that rely on fine image structures, they cannot be applied directly to medium-resolution data due to lack of such fine structures. In this paper, considering the spatial relation of a pixel to its neighborhood, we propose a new deep patch-based CNN system tailored for medium-resolution remote sensing data. The system is designed by incorporating distinctive characteristics of medium-resolution data; in particular, the system computes patch-based samples from multidimensional top of atmosphere reflectance data. With a test site from the Florida Everglades area (with a size of 771 square kilometers), the proposed new system has outperformed pixel-based neural network, pixel-based CNN and patch-based neural network by 24.36{\%}, 24.23{\%} and 11.52{\%}, respectively, in overall classification accuracy. By combining the proposed deep CNN and the huge collection of medium-resolution remote sensing data, we believe that much more accurate land cover datasets can be produced over large areas.},
author = {Sharma, Atharva and Liu, Xiuwen and Yang, Xiaojun and Shi, Di},
doi = {10.1016/j.neunet.2017.07.017},
file = {:opt/pdf/A patch-based convolutional neural network for remote sensingimage classification.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {CNN,Deep learning,Medium-resolution,Patch-based,Remote sensing imagery,Spatial context},
pages = {19--28},
publisher = {Elsevier Ltd},
title = {{A patch-based convolutional neural network for remote sensing image classification}},
url = {http://dx.doi.org/10.1016/j.neunet.2017.07.017},
volume = {95},
year = {2017}
}
@article{Audebert2017a,
abstract = {In this work, we present a novel module to perform fusion of heterogeneous data using fully convolutional networks for semantic labeling. We introduce residual correction as a way to learn how to fuse predictions coming out of a dual stream architecture. Especially, we perform fusion of DSM and IRRG optical data on the ISPRS Vaihingen dataset over a urban area and obtain new state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1701.05818},
author = {Audebert, Nicolas and {Le Saux}, Bertrand and Lef{\`{e}}vrey, S{\'{e}}bastien},
doi = {10.1109/JURSE.2017.7924566},
eprint = {1701.05818},
file = {:opt/pdf/1701.05818.pdf:pdf},
isbn = {9781509058082},
journal = {2017 Joint Urban Remote Sensing Event, JURSE 2017},
title = {{Fusion of heterogeneous data in convolutional networks for urban semantic labeling}},
year = {2017}
}
@article{Hudjakov2009,
abstract = {This article focuses on the problem of terrain classification from aerial imagery with the intention to increase unmanned ground vehicle (UGV) road and off-road performance by providing means to analyze data from unmanned aerial vehicle (UAV).},
author = {Hudjakov, Robert and Tamre, Mart},
doi = {10.1109/ISOT.2009.5326104},
file = {:opt/pdf/Aerial Imagery Terrain Classification for Long-Range Autonomous Navigation.pdf:pdf},
isbn = {9781424442102},
issn = {23466138},
journal = {ISOT 2009 - International Symposium on Optomechatronic Technologies},
keywords = {Convolutional neural networks,Optical terrain classification,UAV,UGV},
pages = {88--91},
title = {{Aerial imagery terrain classification for long-range autonomous navigation}},
year = {2009}
}
@article{Chen2018a,
author = {Chen, Kaiqiang and Fu, Kun and Yan, Menglong and Gao, Xin and Sun, Xian and Wei, Xin},
doi = {10.1109/LGRS.2017.2778181},
file = {:opt/pdf/08246726.pdf:pdf},
issn = {1545598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Aerial images,convolutional neural networks (CNNs),deep learning,remote sensing,semantic segmentation},
number = {2},
pages = {173--177},
title = {{Semantic Segmentation of Aerial Images With Shuffling Convolutional Neural Networks}},
volume = {15},
year = {2018}
}
@article{Liu2016,
author = {Liu, B I N and Zhao, Wencang and Sun, Qiaoqiao},
file = {:opt/pdf/08243900.pdf:pdf},
isbn = {2016083700},
journal = {Ieee},
keywords = {4,added in feature map,and the,by fast r-cnn directly,fast r-cnn,faster r-cnn,network training directly,object dection,proposed box information is,regression to the cnn,rpn,the image is normalized,which is},
pages = {6233--6236},
title = {{Study Of Object Detection Based On Faster R-CNN}},
year = {2016}
}
@article{Kim2017,
abstract = {License plate recognition is an active research field as demands sharply
increase with the development of Intelligent Transportation System
(ITS). However, since the license plate recognition(LPR) is sensitive to
the conditions of the surrounding environment such as a complicated
background in the image, viewing angle and illumination change, it is
still difficult to correctly recognize letters and digits on LPR. This
study applies Deep Convolutional Neural Network (DCNN) to the license
plate recognition. The DCNN is a method of which the performance has
recently been proven to have an excellent generalization error rate in
the field of image recognition. The proposed layer structure of the DCNN
used in this study consists of a combination of a layer for judging the
existence of a license plate and a layer for recognizing digits and
characters. This learning method is based on Multi- Task Learning (MTL).
Through experiments using real images, this study shows that this layer
structure classifies digits and characters more accurately than the DCNN
using a conventional layer does. We also use artificial images generated
directly for training model.},
author = {Kim, Hong-Hyun and Park, Je-Kang and Oh, Joo-Hee and Kang, Dong-Joong},
doi = {10.1007/s12555-016-0332-z},
file = {:opt/pdf/10.1007{\_}s12555-016-0332-z.pdf:pdf},
issn = {1598-6446},
journal = {International Journal of Control, Automation and Systems},
keywords = {deep convolutional neural network,license plate recognition,machine learning,multi task learning},
number = {6},
pages = {2942--2949},
title = {{Multi-task convolutional neural network system for license plate recognition}},
url = {http://link.springer.com/10.1007/s12555-016-0332-z},
volume = {15},
year = {2017}
}
@book{De2016,
author = {De, Sourav and Bhattacharyya, Siddhartha and Chakraborty, Susanta and Dutta, Paramartha},
doi = {10.1007/978-3-319-47524-0},
file = {:opt/pdf/10.1007{\_}978-3-319-47223-2.pdf:pdf},
isbn = {978-3-319-47523-3},
title = {{Hybrid Soft Computing for Multilevel Image and Data Segmentation}},
url = {http://link.springer.com/10.1007/978-3-319-47524-0},
year = {2016}
}
@article{Nogueira2017,
abstract = {We present an analysis of three possible strategies for exploiting the power of existing convolutional neural networks (ConvNets or CNNs) in different scenarios from the ones they were trained: full training, fine tuning, and using ConvNets as feature extractors. In many applications, especially including remote sensing, it is not feasible to fully design and train a new ConvNet, as this usually requires a considerable amount of labeled data and demands high computational costs. Therefore, it is important to understand how to better use existing ConvNets. We perform experiments with six popular ConvNets using three remote sensing datasets. We also compare ConvNets in each strategy with existing descriptors and with state-of-the-art baselines. Results point that fine tuning tends to be the best performing strategy. In fact, using the features from the fine-tuned ConvNet with linear SVM obtains the best results. We also achieved state-of-the-art results for the three datasets used.},
archivePrefix = {arXiv},
arxivId = {1602.01517},
author = {Nogueira, Keiller and Penatti, Ot{\'{a}}vio A.B. and dos Santos, Jefersson A.},
doi = {10.1016/j.patcog.2016.07.001},
eprint = {1602.01517},
file = {:opt/pdf/convolutional neural networks for remotesensing scene classification.pdf:pdf},
isbn = {5531340958},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Aerial scenes,Convolutional neural networks,Deep learning,Feature extraction,Fine-tune,Hyperspectral images,Remote sensing},
pages = {539--556},
pmid = {20015589},
publisher = {Elsevier},
title = {{Towards better exploiting convolutional neural networks for remote sensing scene classification}},
url = {http://dx.doi.org/10.1016/j.patcog.2016.07.001},
volume = {61},
year = {2017}
}
@article{Leibe2016,
author = {Leibe, Bastian},
file = {:opt/pdf/cv16-part15-categorization4.pdf:pdf},
title = {{Computer Vision – Lecture 15 Deep Learning for Object Categorization}},
year = {2016}
}
@article{Detection2011,
author = {Detection, Object},
file = {:opt/pdf/cv16-part08-categorization1.pdf:pdf},
title = {{Computer Vision – Lecture 10 Sliding-Window based Object Detection}},
year = {2011}
}
@article{Yu2017,
abstract = {As a powerful visual model, convolutional neural networks (CNNs) have demonstrated remarkable performance in various visual recognition problems, and attracted considerable attention in recent years. However, due to the highly correlated bands and insufficient training samples of hyperspectral image data, it still remains a challenging problem to effectively apply the CNN models on hyperspectral images. In this paper, an efficient CNN architecture has been proposed to boost its discriminative capability for hyperspectral image classification, in which the original data is used as the input and the final CNN outputs are the predicted class-related results. The proposed CNN infrastructure has several distinct advantages. Firstly, different from traditional classification methods those need hand-crafted features, the CNN model used here is designed to deal with the problem of hyperspectral image analysis in an end-to-end way. Secondly, the parameters of the CNN model are optimized from a small training set, while the over-fitting problem of the neural network has been alleviated to some extent. Finally, in order to better deal with the hyperspectral image information, 1 × 1 convolutional layers have been adopted, and an average pooling layer and larger dropout rates have also been employed in the whole CNN procedure. The experiments on three benchmark data sets have demonstrated that the proposed CNN architecture considerably outperforms other state-of-the-art methods.},
author = {Yu, Shiqi and Jia, Sen and Xu, Chunyan},
doi = {10.1016/j.neucom.2016.09.010},
file = {:opt/pdf/Convolutional neural networks for hyperspectral image classification.pdf:pdf},
isbn = {9781509061822},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Convolutional neural networks,Deep learning,Hyperspectral image classification},
pages = {88--98},
publisher = {Elsevier},
title = {{Convolutional neural networks for hyperspectral image classification}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.09.010},
volume = {219},
year = {2017}
}
@article{Bejiga2016,
abstract = {In recent years, unmanned aerial vehicles (UAVs) have been widely used for civilian remote sensing applications. One of them is to assess damages due to man-made or natural disasters and search for bodies in the debris. In this work, we propose to support avalanche search and rescue (SAR) operation with UAVs. The image acquired by the UAV is processed through a pre-trained convolutional neural network (CNN) to extract discriminative features. A linear support vector machine (SVM) is integrated at the top of the CNN to detect objects of interest. Experimental results show encouraging detection performance at a reasonable processing time.},
author = {Bejiga, M B and Zeggada, A and Melgani, F},
doi = {10.1109/IGARSS.2016.7729174},
file = {:opt/pdf/Convolutional neural networks for near real-time object detection from UAV imagery in avalanche search and rescue operations.pdf:pdf},
isbn = {9781509033324},
journal = {2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
keywords = {Avalanche,Convolutional Neural Networks (CNN),Feature extraction,Neural networks,Object Detection,Rescue,Snow,Support Vector Machines (SVM),Support vector machines,Synthetic aperture radar,Training,UAV imagery,Unmanned Aerial Vehicle (UAV),Unmanned aerial vehicles,autonomous aerial vehicles,avalanche search-and-rescue operation,civilian remote sensing application,convolutional neural networks,detection performance,disasters,discriminative feature extraction,emergency management,feature extraction,geophysical image processing,linear SVM,man-made disaster,natural disaster,neural nets,object detection,real-time object detection,remote sensing,support vector machine,support vector machines,unmanned aerial vehicle},
pages = {693--696},
title = {{Convolutional neural networks for near real-time object detection from UAV imagery in avalanche search and rescue operations}},
year = {2016}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:opt/pdf/arxive2013.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@article{Bowley2017,
author = {Bowley, Connor and Mattingly, Marshall and Barnas, Andrew and Ellis-Felege, Susan and Desell, Travis},
doi = {10.1109/eScience.2017.22},
file = {:opt/pdf/citizen scientist to drive automated ecological object detection.pdf:pdf},
isbn = {9781538626863},
journal = {Proceedings - 13th IEEE International Conference on eScience, eScience 2017},
keywords = {Crowdsourcing,Learning systems,Machine learning,Neural networks},
pages = {99--108},
title = {{Toward using citizen scientists to drive automated ecological object detection in aerial imagery}},
year = {2017}
}
@article{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {1603.05027},
file = {:opt/pdf/1603.05027.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {630--645},
pmid = {23554596},
title = {{Identity mappings in deep residual networks}},
volume = {9908 LNCS},
year = {2016}
}
@article{Sun2015,
author = {Sun, Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian},
file = {:opt/pdf/kaiminhe.pdf:pdf},
journal = {arXiv},
number = {03385},
title = {{Deep Residual Learning for Image Recognition}},
volume = {1512},
year = {2015}
}
@report{eucomission,
author = {Deudero, S. and Alomar, C},
booktitle = {Marine Pollution Bulletin},
doi = {10.1016/j.marpolbul.2 015.07.012},
institution = {SCU, The University of West England, Bristol},
title = {{Science for Environmental Policy Extent of plastics in the Mediterranean Sea: a growing problem}},
url = {http://ec.europa.eu/environment/integration/research/newsalert/pdf/extent{\_}of{\_}plastics{\_}in{\_}the{\_}mediterranean{\_}sea{\_}a{\_}growing{\_}problem{\_}435na5{\_}en.pdf},
year = {2015}
}
@article{mathworks,
title = {{Convolutional Neural Networks - Mathworks and Simulink}},
url = {https://www.mathworks.com/discovery/convolutional-neural-network.html}
}
@article{suaria2014floating,
author = {Suaria, Giuseppe and Aliani, Stefano},
journal = {Marine pollution bulletin},
number = {1-2},
pages = {494--504},
publisher = {Elsevier},
title = {{Floating debris in the Mediterranean Sea}},
volume = {86},
year = {2014}
}
@article{deudero2015mediterranean,
author = {Deudero, Salud and Alomar, Carme},
journal = {Marine Pollution Bulletin},
number = {1-2},
pages = {58--68},
publisher = {Elsevier},
title = {{Mediterranean marine biodiversity under threat: reviewing influence of marine litter on species}},
volume = {98},
year = {2015}
}
@article{pham2014marine,
author = {Pham, Christopher K and Ramirez-Llodra, Eva and Alt, Claudia H S and Amaro, Teresa and Bergmann, Melanie and Canals, Miquel and Davies, Jaime and Duineveld, Gerard and Galgani, Fran{\c{c}}ois and Howell, Kerry L and Others},
journal = {PloS one},
number = {4},
pages = {e95839},
publisher = {Public Library of Science},
title = {{Marine litter distribution and density in European seas, from the shelves to deep basins}},
volume = {9},
year = {2014}
}
@article{barnes2009accumulation,
author = {Barnes, David K A and Galgani, Francois and Thompson, Richard C and Barlaz, Morton},
journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
number = {1526},
pages = {1985--1998},
publisher = {The Royal Society},
title = {{Accumulation and fragmentation of plastic debris in global environments}},
volume = {364},
year = {2009}
}
@article{galgani2000litter,
author = {Galgani, F and Leaute, J P and Moguedet, P and Souplet, A and Verin, Y and Carpentier, A and Goraguer, H and Latrouite, D and Andral, B and Cadiou, Y and Others},
journal = {Marine pollution bulletin},
number = {6},
pages = {516--527},
publisher = {Elsevier},
title = {{Litter on the sea floor along European coasts}},
volume = {40},
year = {2000}
}
@article{PreveniosMichael2018Bldo,
author = {Prevenios, Michael and Zeri, Christina and Tsangaris, Catherine and Liubartseva, Svitlana and Fakiris, Elias and Papatheodorou, George},
issn = {0025-326X},
journal = {Marine Pollution Bulletin},
keywords = {Marine Debris ; Plastics ; Net Accumulation Rates},
month = {apr},
number = {2},
pages = {448--457},
publisher = {Elsevier Ltd},
title = {{Beach litter dynamics on Mediterranean coasts: Distinguishing sources and pathways}},
volume = {129},
year = {2018}
}
@article{Girshick2014a,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:opt/pdf/1311.2524.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@book{Dai2016,
abstract = {We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6{\%} mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn},
archivePrefix = {arXiv},
arxivId = {1605.06409},
author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
doi = {10.1109/ICASSP.2017.7952132},
eprint = {1605.06409},
file = {:opt/pdf/1605.06409.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
pages = {379--387},
pmid = {23739795},
title = {{R-FCN: Object Detection via Region-based Fully Convolutional Networks}},
url = {http://arxiv.org/abs/1605.06409},
year = {2016}
}
@article{Ren2017b,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:opt/pdf/1506.01497.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}
@article{Wu2017,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Wu, Songtao and Zhong, Shenghua and Liu, Yan},
doi = {10.1007/s11042-017-4440-4},
eprint = {1512.03385},
file = {:opt/pdf/1512.03385.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
pages = {1--17},
pmid = {23554596},
title = {{Deep residual learning for image steganalysis}},
year = {2017}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
doi = {10.1016/j.patrec.2014.01.008},
eprint = {1602.07261},
file = {:opt/pdf/1602.07261.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
pmid = {23064159},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{Liu2016a,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:opt/pdf/1512.02325.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
pages = {21--37},
pmid = {23739795},
title = {{SSD: Single shot multibox detector}},
volume = {9905 LNCS},
year = {2016}
}
@misc{Hill2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hill, H.},
booktitle = {Phi Delta Kappan},
doi = {10.1177/0964663912467814},
eprint = {arXiv:1011.1669v3},
file = {:opt/pdf/1512.02325.pdf:pdf},
isbn = {0824701712},
issn = {09646639},
pages = {470--476},
pmid = {877579},
title = {{Fixing Teacher Professional Development}},
volume = {90},
year = {2009}
}
@article{Sommer2017,
abstract = {Growing cities and increasing traffic densities result in an increased demand for applications such as traffic monitoring, traffic analysis, and support of rescue work. These applications share the need for accurate detection of relevant vehicles, e.g. in aerial imagery. Recently, the application of deep learning based detection frameworks like Faster R-CNN clearly outperformed conventional detection methods for vehicle detection in aerial images. In this paper, we propose a detection framework that fuses Faster R-CNN and semantic labeling to integrate contextual information. We achieve an improved detection performance by decreasing the number of false positive detections while the number of candidate regions to classify is reduced. To demonstrate the generalization of our approach, we evaluate our detection framework for various ground sampling distances on a publicly available dataset.},
author = {Sommer, Lars and Nie, Kun and Schumann, Arne and Schuchert, Tobias and Beyerer, Jurgen},
doi = {10.1109/AVSS.2017.8078510},
file = {:opt/pdf/08078510.pdf:pdf},
isbn = {9781538629390},
journal = {2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2017},
pages = {1--6},
title = {{Semantic labeling for improved vehicle detection in aerial imagery}},
year = {2017}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:opt/pdf/1409.1556.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{David2014a,
author = {David, S},
file = {:home/mike/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/David - 2014 - a Comparative Study of Various.pdf:pdf},
isbn = {9781538618875},
journal = {2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)},
keywords = {dct,dwt,harr wavelets,inverse radon transform,lifting schemes,radon transform,sphit},
number = {4},
pages = {16--20},
publisher = {IEEE},
title = {{a Comparative Study of Various Image}},
volume = {3},
year = {2014}
}
@book{Taylor2014,
archivePrefix = {arXiv},
arxivId = {cs.CV/1804.03928},
author = {Taylor, Graham and Ranzato, Marc' Aurelio and Lee, Honglak},
booktitle = {Computer Vision and Pattern Recognition Workshop},
eprint = {1804.03928},
file = {:opt/pdf/DEEP{\_}LEARNING{\_}FOR{\_}COMPUTER{\_}VISION.pdf:pdf},
isbn = {9781450334730},
pages = {1--46},
primaryClass = {cs.CV},
title = {{Deep Learning for Computer Vision}},
year = {2014}
}
@article{MasMontserrat2017,
author = {{Mas Montserrat}, Daniel and Lin, Qian and Allebach, Jan and Delp, EdwardJ J and Lafayette, West and Lin, Qian and Alto, Palo and Allebach, Jan and Lafayette, West and Delp, EdwardJ J and Lafayette, West},
doi = {doi:10.2352/ISSN.2470-1173.2017.10.IMAWM-163},
file = {:opt/pdf/dataugment.pdf:pdf},
journal = {Electronic Imaging},
pages = {27--36},
title = {{Training Object Detection And Recognition CNN Models Using Data Augmentation}},
volume = {2017},
year = {2017}
}
@article{Kampffmeyer2016,
author = {Kampffmeyer, Michael and Salberg, Arnt-Borre and Jenssen, Robert},
doi = {10.1109/CVPRW.2016.90},
file = {:opt/pdf/07789580.pdf:pdf},
isbn = {978-1-5090-1437-8},
issn = {21607516},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
pages = {680--688},
title = {{Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban Remote Sensing Images Using Deep Convolutional Neural Networks}},
url = {http://ieeexplore.ieee.org/document/7789580/},
year = {2016}
}
@article{Sep2016a,
abstract = {— An accurate and reliable image based fruit detec-tion system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. This paper presents the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Abla-tion studies are presented to better understand the practical deployment of the detection network, including how much training data is required to capture variability in the dataset. Data augmentation techniques are shown to yield significant performance gains, resulting in a greater than two-fold reduc-tion in the number of training images required. In contrast, transferring knowledge between orchards contributed to negli-gible performance gain over initialising the Deep Convolutional Neural Network directly from ImageNet features. Finally, to operate over orchard data containing between 100-1000 fruit per image, a tiling approach is introduced for the Faster R-CNN framework. The study has resulted in the best yet detection performance for these orchards relative to previous works, with an F1-score of {\textgreater} 0.9 achieved for apples and mangoes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.03677v2},
author = {Sep, R O},
eprint = {arXiv:1610.03677v2},
file = {:opt/pdf/07989417.pdf:pdf},
isbn = {9781509046331},
pages = {3626--3633},
title = {{Deep Fruit Detection in Orchards}},
year = {2016}
}
@article{Bu2017,
author = {Bu, Qi and Lan, Shanzhen and Xu, Pin},
doi = {10.1145/3094243.3094258},
file = {:opt/pdf/p86-Bu.pdf:pdf},
isbn = {9781450352321},
journal = {Proceedings of the 2017 International Conference on Deep Learning Technologies  - ICDLT '17},
keywords = {car model recognition,image preprocessing,improved caffenet},
number = {1},
pages = {86--89},
title = {{A CNN based car model recognition improvement}},
url = {http://dl.acm.org/citation.cfm?doid=3094243.3094258},
year = {2017}
}
@article{Salberg2015,
abstract = {In this paper, we propose an algorithm for automatic detection of seals in aerial remote sensing images using features ex- tracted from a pre-trained deep convolutional neural network (CNN). The method consists of three stages: (i) Detection of potential objects, (ii) feature extraction and (iii) classification of potential objects. The first stage is application dependent, with the aim of detecting all seal pups in the image, with the expense of detecting a large amount of false objects. The sec- ond stage extracts generic image features from a local image corresponding to each potential seal detected in the first stage using a CNN trained on the ImageNet database. In the third stage we apply a linear support vector machine to classify the feature vectors extracted in the second stage. The proposed method was demonstrated to an aerial image that contains 84 pups and 128 adult harp seals, and the results show that we are able to detect the seals with high accuracy (2.7{\%} for the adults and 7.3{\%} for the pups). We conclude that deep CNNs trained on the ImageNet database are well suited as a feature extraction module, and using a simple linear SVM, we were able to separate seals from other objects with very high ac- curacy. We believe that this methodology may be applied to other other remote sensing object recognition tasks.},
author = {Salberg, Arnt-Borre},
doi = {10.1109/IGARSS.2015.7326163},
file = {:opt/pdf/07326163.pdf:pdf},
isbn = {9781479979295},
issn = {2153-6996},
journal = {2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
number = {0373},
pages = {1893--1896},
title = {{Detection of seals in remote sensing images using features extracted from deep convolutional neural networks}},
year = {2015}
}
@article{Sherrah2016,
abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1606.02585},
author = {Sherrah, Jamie},
eprint = {1606.02585},
file = {:opt/pdf/1606.02585.pdf:pdf},
isbn = {9781509033324},
pages = {1--22},
title = {{Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery}},
url = {http://arxiv.org/abs/1606.02585},
year = {2016}
}
@article{Rezaee2018a,
author = {Rezaee, Mohammad and Mahdianpari, Masoud and Zhang, Yun and Salehi, Bahram},
doi = {10.1109/JSTARS.2018.2846178},
file = {:opt/pdf/08401505.pdf:pdf},
issn = {1939-1404},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
pages = {1--10},
publisher = {IEEE},
title = {{Deep Convolutional Neural Network for Complex Wetland Classification Using Optical Remote Sensing Imagery}},
url = {https://ieeexplore.ieee.org/document/8401505/},
volume = {PP},
year = {2018}
}
@article{Radovic2017,
abstract = {There are numerous applications of unmanned aerial vehicles (UAVs) in the management of civil infrastructure assets. A few examples include routine bridge inspections, disaster management, power line surveillance and traffic surveying. As UAV applications become widespread, increased levels of autonomy and independent decision-making are necessary to improve the safety, efficiency, and accuracy of the devices. This paper details the procedure and parameters used for the training of convolutional neural networks (CNNs) on a set of aerial images for efficient and automated object recognition. Potential application areas in the transportation field are also highlighted. The accuracy and reliability of CNNs depend on the network's training and the selection of operational parameters. This paper details the CNN training procedure and parameter selection. The object recognition results show that by selecting a proper set of parameters, a CNN can detect and classify objects with a high level of accuracy (97.5{\%}) and computational efficiency. Furthermore, using a convolutional neural network implemented in the “YOLO” (“You Only Look Once”) platform, objects can be tracked, detected (“seen”), and classified (“comprehended”) from video feeds supplied by UAVs in real-time.},
author = {Radovic, Matija and Adarkwa, Offei and Wang, Qiaosong},
doi = {10.3390/jimaging3020021},
file = {:opt/pdf/jimaging-03-00021.pdf:pdf},
issn = {2313-433X},
journal = {Journal of Imaging},
keywords = {convolutional neural networks,object recognition,uav,unmanned aerial vehicle},
number = {2},
pages = {21},
title = {{Object Recognition in Aerial Images Using Convolutional Neural Networks}},
url = {http://www.mdpi.com/2313-433X/3/2/21},
volume = {3},
year = {2017}
}
@article{Malof2016a,
author = {Malof, Jordan M and Bradbury, Kyle and Collins, Leslie M. and Newell, Richard G.},
doi = {10.1109/ICRERA.2016.7884415},
file = {:opt/pdf/07884415.pdf:pdf},
isbn = {9781509033881},
journal = {International Conference on Renewable Energy Research and Applications (ICRERA)},
keywords = {-component,1hzhoo,5lfkdug,convolutional neural networks,deep,detection,energy,learning,oh,photovoltaic,qhuj,qlwldwlyh,solar,udgexu,xnh 8qlyhuvlw},
pages = {650--654},
title = {{A Deep Convolutional Neural Network and a Random Forest Classifier for Solar Photovoltaic Array Detection in Aerial Imagery}},
volume = {5},
year = {2016}
}
@article{Cheng2016,
abstract = {Thanks to the powerful feature representations obtained through deep convolutional neural network (CNN), the performance of object detection has recently been substantially boosted. Despite the remarkable success, the problems of object rotation, within-class variability, and between-class similarity remain several major challenges. To address these problems, this paper proposes a novel and effective method to learn a rotation-invariant and Fisher discriminative CNN (RIFD-CNN) model. This is achieved by introducing and learning a rotation-invariant layer and a Fisher discriminative layer, respectively, on the basis of the existing high-capacity CNN architectures. Specifically, the rotation-invariant layer is trained by imposing an explicit regularization constraint on the objective function that enforces invariance on the CNN features before and after rotating. The Fisher discriminative layer is trained by imposing the Fisher discrimination criterion on the CNN features so that they have small within-class scatter but large between-class separation. In the experiments, we comprehensively evaluate the proposed method for object detection task on a public available aerial image dataset and the PASCAL VOC 2007 dataset. State-of-the-art results are achieved compared with the existing baseline methods.},
author = {Cheng, Gong and Zhou, Peicheng and Han, Junwei},
doi = {10.1109/CVPR.2016.315},
file = {:opt/pdf/07780684.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2884--2893},
title = {{RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection}},
url = {http://ieeexplore.ieee.org/document/7780684/},
year = {2016}
}
@article{Yang2017,
author = {Yang, siuhan Lexie and And, Dalton Lunga and Yuan, Jiangye},
file = {:opt/pdf/08127091.pdf:pdf},
isbn = {9781509049516},
pages = {870--873},
title = {{TOWARD COUNTRY SCALE BUILDING DETECTION WITH CONVOLUTIONAL NEURAL NETWORK USING AERIAL IMAGES Hsiuhan Lexie Yang , Dalton Lunga and Jiangye Yuan Computing and Computational Sciences Directorate Oak Ridge National Laboratory}},
year = {2017}
}
@article{Malof2017,
abstract = {{\textcopyright} 2017 IEEE. In this work we consider the problem of developing algorithms that automatically identify small-scale solar photovoltaic arrays in high resolution aerial imagery. Such algorithms potentially offer a faster and cheaper solution to collecting small-scale photovoltaic (PV) information, such as their location, capacity, and the energy they produce. Here we build on previous algorithmic work by employing convolutional neural networks (CNNs), which have recently yielded major improvements in other image object recognition problems. We propose a CNN architecture for our recognition problem and then measure its detection performance on the same (publicly available) dataset that was used in previous publications. The results indicate that the CNN yields substantial performance improvements over previous results. We also investigate the recently popular approach of pre-training for CNNs.},
author = {Malof, J.M. Jordan M. and Collins, Leslie M. L.M. and Bradbury, Kyle},
doi = {10.1109/IGARSS.2017.8127092},
file = {:opt/pdf/08127092.pdf:pdf},
isbn = {9781509049516},
journal = {International conference on geoscience and remote sensing (under review)},
keywords = {image recognition,object detection,photovoltaic,satellite imagery,solar energy},
pages = {2--5},
title = {{A Deep Convolutional Neural Network, with pre-training, for solar photovoltaic array detection in aerial imagery}},
volume = {2017-July},
year = {2017}
}
@article{Nor2010,
author = {Nor, Siti and Binti, Khuzaimah},
file = {:opt/pdf/08228593.pdf:pdf},
pages = {239--245},
title = {{Disaster Detection from Aerial Imagery with Convolutional Neural Network}},
year = {2010}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:opt/pdf/long{\_}shelhamer{\_}fcn.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{Nie2018,
author = {Nie, Kun and Sommer, Lars and Schumann, Arne and Beyerer, Jurgen},
doi = {10.1109/WACV.2018.00074},
file = {:opt/pdf/08354178.pdf:pdf},
isbn = {978-1-5386-4886-5},
journal = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
pages = {626--634},
title = {{Semantic Labeling Based Vehicle Detection in Aerial Imagery}},
url = {https://ieeexplore.ieee.org/document/8354178/},
year = {2018}
}
@article{Wei2017,
abstract = {In this letter, we propose a road structure refined convolutional neural network (RSRCNN) approach for road extraction in aerial images. In order to obtain structured output of road extraction, both deconvolutional and fusion layers are designed in the architecture of RSRCNN. For training RSRCNN, a new loss function is proposed to incorporate the geometric information of road structure in cross-entropy loss, thus called road-structure-based loss function. Experimental results demonstrate that the trained RSRCNN model is able to advance the state-of-the-art road extraction for aerial images, in terms of precision, recall, F-score, and accuracy.},
author = {Wei, Yanan and Wang, Zulin and Xu, Mai},
doi = {10.1109/LGRS.2017.2672734},
file = {:opt/pdf/07876793.pdf:pdf},
issn = {1545598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Convolutional neural network (CNN),machine learning,road extraction},
number = {5},
pages = {709--713},
title = {{Road Structure Refined CNN for Road Extraction in Aerial Image}},
volume = {14},
year = {2017}
}
@article{Tudorache2017,
author = {Tudorache, Silvia and Popescu, Dan and Ichim, Loretta},
file = {:opt/pdf/08170631.pdf:pdf},
isbn = {9781538620595},
keywords = {3,52 billion euros,artificial neural network,due to,flooding,floods amounted to about,hog,identification of,interest,over a period of,regions of,ten years,texture analysis,total losses in europe,uav images,vegetation},
title = {{Combining Efficient Textural Features with CNN – based Classifiers to Segment Regions of Interest in Aerial Images}},
year = {2017}
}
@article{Sommer2018,
author = {Sommer, Lars and Schumann, Arne},
doi = {10.1109/WACV.2018.00075},
file = {:opt/pdf/08354179.pdf:pdf},
isbn = {9781538648865},
title = {{Multi Feature Deconvolutional Faster R-CNN for Precise Vehicle Detection in Aerial Imagery}},
year = {2018}
}
